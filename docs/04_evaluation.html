<!DOCTYPE html><html><head>
      <title>04_evaluation</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:///c:\Users\Quang Chien\.vscode\extensions\shd101wyy.markdown-preview-enhanced-0.8.20\crossnote\dependencies\katex\katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="evaluation-framework">Evaluation Framework </h1>
<p>Comprehensive evaluation framework for the Multimodal Recommendation System (MRS) project.</p>
<hr>
<h2 id="table-of-contents">Table of Contents </h2>
<ol>
<li><a href="#1-research-questions">Research Questions</a></li>
<li><a href="#2-evaluation-metrics">Evaluation Metrics</a></li>
<li><a href="#3-experiment-setup">Experiment Setup</a></li>
<li><a href="#4-experiment-results">Experiment Results</a></li>
<li><a href="#5-in-depth-analysis">In-Depth Analysis</a></li>
<li><a href="#6-conclusion">Conclusion</a></li>
</ol>
<hr>
<h2 id="1-research-questions">1. Research Questions </h2>
<h3 id="11-primary-research-questions">1.1 Primary Research Questions </h3>
<table>
<thead>
<tr>
<th>ID</th>
<th>Research Question</th>
<th>Formalized Hypothesis</th>
<th>Validation Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RQ1</strong></td>
<td><strong>Modality Sensitivity:</strong> To what extent do visual and textual modalities contribute to the predictive performance of MRS across diverse product categories?</td>
<td>We hypothesize that the visual modality exhibits a stronger inductive bias in aesthetic-centric domains (e.g., Clothing, Beauty), whereas the textual modality provides superior disambiguation in functional domains (e.g., Electronics), necessitating modality-specific ablation.</td>
<td><strong>Component-Level Ablation:</strong> Evaluate model performance (ΔNDCG@20) by masking <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (visual) and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> (textual) features independently across domains.</td>
</tr>
<tr>
<td><strong>RQ2</strong></td>
<td><strong>Cold-Start Mechanics:</strong> How does the efficacy of Generative Graph Diffusion (DiffMM) compare to Latent Structure Mining (LATTICE, MICRO) in addressing the Item Cold-Start problem?</td>
<td>While DiffMM mitigates user-interaction sparsity via diffusion-based augmentation, we hypothesize that LATTICE and MICRO will demonstrate superior robustness for cold-start items by explicitly leveraging item-item semantic graphs, which are independent of user interaction history.</td>
<td><strong>Zero-Shot Evaluation:</strong> Comparative benchmarking on the "Cold-Start Item" track (Track 3) vs. the "Warm-Start" track (Track 1), measuring the degradation gap.</td>
</tr>
<tr>
<td><strong>RQ3</strong></td>
<td><strong>Architectural Trade-offs:</strong> What is the performance trade-off between deterministic graph learning and probabilistic generative modeling in terms of ranking accuracy and training stability?</td>
<td>We hypothesize that DiffMM achieves state-of-the-art accuracy in warm-start scenarios by recovering the user-item interaction manifold, whereas MICRO offers the most stable convergence and robust representations through its contrastive modality alignment.</td>
<td><strong>Global Benchmarking:</strong> Cross-model evaluation of Recall@20 and NDCG@20 on the full Amazon Review 2023 dataset, incorporating convergence analysis.</td>
</tr>
<tr>
<td><strong>RQ4</strong></td>
<td><strong>Alignment Correlation:</strong> Does the intrinsic semantic alignment between item modalities dictate the optimal architectural choice?</td>
<td>We hypothesize that datasets with high Canonical Correlation (CCA) between modalities favor MICRO's contrastive objective, while datasets with weak alignment benefit from LATTICE's disjoint structure learning, which learns independent topology per modality.</td>
<td><strong>Correlation Analysis:</strong> Compute Pearson correlation between dataset-specific EDA metrics (e.g., Modal Alignment Score) and model performance (NDCG@20).</td>
</tr>
</tbody>
</table>
<h3 id="12-secondary-research-questions">1.2 Secondary Research Questions </h3>
<table>
<thead>
<tr>
<th>ID</th>
<th>Research Question</th>
<th>Motivation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>RQ5</strong></td>
<td>How does user sparsity affect multimodal recommendation performance?</td>
<td>Sparse users rely more on content features; active users have sufficient collaborative signal.</td>
</tr>
</tbody>
</table>
<h3 id="13-hypotheses-based-on-eda-findings">1.3 Hypotheses Based on EDA Findings </h3>
<p>Based on the exploratory data analysis, we formulate the following domain-specific hypotheses:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Observation from EDA</th>
<th>Hypothesis</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Beauty</strong></td>
<td>Visual alignment r = -0.0009, Text alignment r = 0.025</td>
<td>Text features marginally outperform visual for Beauty products</td>
</tr>
<tr>
<td><strong>Clothing</strong></td>
<td>Visual alignment r = 0.019, Text alignment r = -0.006</td>
<td>Visual features are the primary signal for Clothing recommendations</td>
</tr>
<tr>
<td><strong>Electronics</strong></td>
<td>Visual alignment r = 0.016, Text alignment r = 0.018</td>
<td>Both modalities contribute equally for Electronics</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note:</strong> All datasets show weak direct alignment (|r| &lt; 0.05), indicating that simple cosine similarity does not capture user preference patterns. This motivates the use of learned representations (LATTICE/MICRO/DiffMM) over raw feature similarity.</p>
</blockquote>
<hr>
<h2 id="2-evaluation-metrics">2. Evaluation Metrics </h2>
<h3 id="21-ranking-metrics">2.1 Ranking Metrics </h3>
<p>The evaluation framework implements <strong>all-ranking evaluation</strong>, computing metrics over the entire item catalog (not sampled negatives).</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Formula</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Recall@K</strong></td>
<td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">{</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>∈</mo><mtext>TopK</mtext><mo>∩</mo><mi>i</mi><mo>∈</mo><mtext>GT</mtext><mo stretchy="false">}</mo><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mtext>GT</mtext><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{|\{i : i \in \text{TopK} \cap i \in \text{GT}\}|}{|\text{GT}|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord text mtight"><span class="mord mtight">GT</span></span><span class="mord mtight">∣</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mopen mtight">{</span><span class="mord mathnormal mtight">i</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord mtight">TopK</span></span><span class="mbin mtight">∩</span><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord mtight">GT</span></span><span class="mclose mtight">}</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></td>
<td>Fraction of relevant items retrieved in top-K</td>
</tr>
<tr>
<td><strong>NDCG@K</strong></td>
<td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mtext>DCG</mtext><mi mathvariant="normal">@</mi><mi>K</mi></mrow><mrow><mtext>IDCG</mtext><mi mathvariant="normal">@</mi><mi>K</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\text{DCG}@K}{\text{IDCG}@K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">IDCG</span></span><span class="mord mtight">@</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">DCG</span></span><span class="mord mtight">@</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>DCG</mtext><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mfrac><mrow><mn mathvariant="double-struck">1</mn><mo stretchy="false">[</mo><mi>i</mi><mo>∈</mo><mtext>GT</mtext><mo stretchy="false">]</mo></mrow><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{DCG} = \sum_{i=1}^{K} \frac{\mathbb{1}[i \in \text{GT}]}{\log_2(i+1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">DCG</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.5411em;vertical-align:-0.5311em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop mtight"><span class="mtight">l</span><span class="mtight">o</span><span class="mtight" style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span style="top:-2.2341em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2659em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mopen mtight">[</span><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord mtight">GT</span></span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5311em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></td>
<td>Position-aware ranking quality</td>
</tr>
<tr>
<td><strong>Precision@K</strong></td>
<td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">{</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>∈</mo><mtext>TopK</mtext><mo>∩</mo><mi>i</mi><mo>∈</mo><mtext>GT</mtext><mo stretchy="false">}</mo><mi mathvariant="normal">∣</mi></mrow><mi>K</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{|\{i : i \in \text{TopK} \cap i \in \text{GT}\}|}{K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mopen mtight">{</span><span class="mord mathnormal mtight">i</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord mtight">TopK</span></span><span class="mbin mtight">∩</span><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord mtight">GT</span></span><span class="mclose mtight">}</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></td>
<td>Fraction of top-K items that are relevant</td>
</tr>
</tbody>
</table>
<h3 id="22-k-values">2.2 K Values </h3>
<p>Based on typical recommendation system evaluation practices:</p>
<table>
<thead>
<tr>
<th>K</th>
<th>Use Case</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>Short list</td>
<td>Typical mobile/widget display</td>
</tr>
<tr>
<td><strong>20</strong></td>
<td><strong>Primary metric</strong></td>
<td>Standard benchmark comparison</td>
</tr>
<tr>
<td>50</td>
<td>Long list</td>
<td>Web/catalog browsing</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Primary Metric:</strong> <code>Recall@20</code> is used for early stopping and model selection, following conventions in LATTICE and MICRO papers.</p>
</blockquote>
<h3 id="23-metric-computation-details">2.3 Metric Computation Details </h3>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token comment"># Implementation from src/evaluator.py</span>
<span class="token keyword keyword-def">def</span> <span class="token function">compute_metrics</span><span class="token punctuation">(</span>scores<span class="token punctuation">,</span> ground_truth<span class="token punctuation">,</span> train_positive<span class="token punctuation">,</span> k_list<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    All-ranking evaluation with training item masking.
    
    Args:
        scores: (n_users, n_items) prediction scores
        ground_truth: {user_idx: set(item_idx)} test positive items
        train_positive: {user_idx: set(item_idx)} training items to mask
        k_list: List of K values [10, 20, 50]
    """</span>
    <span class="token comment"># Mask training items (set to -inf)</span>
    <span class="token keyword keyword-for">for</span> user<span class="token punctuation">,</span> items <span class="token keyword keyword-in">in</span> train_positive<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        scores<span class="token punctuation">[</span>user<span class="token punctuation">,</span> items<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span>inf
    
    <span class="token comment"># Get top-K items and compute metrics</span>
    topk_indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token punctuation">(</span>k_list<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token comment"># ... compute Recall, NDCG, Precision</span>
</code></pre><hr>
<h2 id="3-experiment-setup">3. Experiment Setup </h2>
<h3 id="31-three-track-evaluation-protocol">3.1 Three-Track Evaluation Protocol </h3>
<p>The evaluation framework implements a comprehensive <strong>three-track protocol</strong> to assess model performance across different scenarios. Understanding why multiple evaluation tracks are necessary is crucial for interpreting our experimental results.</p>
<h4 id="understanding-warm-vs-cold-evaluation">Understanding Warm vs. Cold Evaluation </h4>
<p>In recommender systems, we distinguish between two fundamentally different scenarios:</p>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Definition</th>
<th>Challenge</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Warm Items</strong></td>
<td>Items that appear in the training data with sufficient user interactions. The model has learned ID-based embeddings for these items.</td>
<td>Leveraging collaborative filtering signals effectively.</td>
</tr>
<tr>
<td><strong>Cold Items</strong></td>
<td>Items that <strong>never appear in training data</strong> (zero interactions). The model must rely entirely on content features (images, text) to represent these items.</td>
<td>Generalizing to unseen items without historical signals.</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!NOTE]<br>
<strong>Why is cold-start important?</strong> In real-world e-commerce, new products are continuously added to catalogs. A recommender that cannot handle cold items will fail to promote new inventory, creating a "rich-get-richer" effect where only established products receive exposure.</p>
</blockquote>
<p>Similarly, users can be classified by their interaction density:</p>
<table>
<thead>
<tr>
<th>User Type</th>
<th>Definition</th>
<th>Implication</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Active Users</strong> (≥20 interactions)</td>
<td>Users with rich purchase/rating history. The model has strong collaborative signals.</td>
<td>Easier to model preferences.</td>
</tr>
<tr>
<td><strong>Sparse Users</strong> (≤5 interactions)</td>
<td>Users with minimal history. The model must rely more on content-based inference.</td>
<td>Harder to infer preferences—similar to "cold users."</td>
</tr>
</tbody>
</table>
<h4 id="rationale-for-three-evaluation-tracks">Rationale for Three Evaluation Tracks </h4>
<p>We evaluate on three distinct tracks to answer different research questions:</p>
<table>
<thead>
<tr>
<th>Track</th>
<th>What It Measures</th>
<th>Why It Matters</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Track 1: Warm Performance</strong></td>
<td>Standard recommendation quality on items seen during training.</td>
<td>Baseline comparison—ensures models can compete with traditional collaborative filtering.</td>
</tr>
<tr>
<td><strong>Track 2: User Robustness</strong></td>
<td>Performance gap between sparse and active users on warm items.</td>
<td>Tests whether models disproportionately favor "easy" users with rich history. A fair recommender should not abandon sparse users.</td>
</tr>
<tr>
<td><strong>Track 3: Cold-Start</strong></td>
<td>Recommendation quality on items <strong>never seen during training</strong>.</td>
<td>Tests inductive capability—can the model transfer learned representations to completely new items using only visual/text features?</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!IMPORTANT]<br>
<strong>Track 3 is an inductive evaluation.</strong> Cold items have no ID embeddings from training. Each model must generate item representations purely from multimodal features (CLIP visual embeddings, SBERT text embeddings). This is a fundamentally harder task than Track 1.</p>
</blockquote>
<h4 id="track-summary-table">Track Summary Table </h4>
<table>
<thead>
<tr>
<th>Track</th>
<th>Split</th>
<th>Filter</th>
<th>Purpose</th>
<th>Metric Focus</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Track 1: Warm</strong></td>
<td>test_warm.txt</td>
<td>All users</td>
<td>Standard warm-start performance</td>
<td>Recall@K, NDCG@K</td>
</tr>
<tr>
<td><strong>Track 2a: Sparse Users</strong></td>
<td>test_warm.txt</td>
<td>Users with ≤5 interactions</td>
<td>User robustness (sparse user fairness)</td>
<td>Recall@20</td>
</tr>
<tr>
<td><strong>Track 2b: Active Users</strong></td>
<td>test_warm.txt</td>
<td>Users with ≥20 interactions</td>
<td>Active user performance (upper bound)</td>
<td>Recall@20</td>
</tr>
<tr>
<td><strong>Track 3: Cold-Start</strong></td>
<td>test_cold.txt</td>
<td>All users, inductive mode</td>
<td>True cold-start (unseen items)</td>
<td>Recall@K</td>
</tr>
</tbody>
</table>
<h3 id="32-data-split-strategy">3.2 Data Split Strategy </h3>
<pre data-role="codeBlock" data-info="" class="language-text"><code>Raw 5-core CSV → Seed Sampling → Recursive k-Core → Warm/Cold Item Split → Temporal Train/Val/Test
</code></pre><p><strong>Configuration Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>seed_users</code></td>
<td>10,000</td>
<td>Initial user sample for tractable experiments</td>
</tr>
<tr>
<td><code>k_core</code></td>
<td>5</td>
<td>Minimum interactions per user/item</td>
</tr>
<tr>
<td><code>cold_item_ratio</code></td>
<td>0.20</td>
<td>20% of items held out as cold</td>
</tr>
<tr>
<td><code>train_ratio</code></td>
<td>0.80</td>
<td>Training set from warm interactions</td>
</tr>
<tr>
<td><code>val_ratio</code></td>
<td>0.10</td>
<td>Validation set for early stopping</td>
</tr>
<tr>
<td><code>test_warm_ratio</code></td>
<td>0.10</td>
<td>Test set for warm evaluation</td>
</tr>
</tbody>
</table>
<p><strong>Item ID Structure:</strong></p>
<ul>
<li>Warm Items: IDs <code>[0, N_warm - 1]</code></li>
<li>Cold Items: IDs <code>[N_warm, N_total - 1]</code></li>
</ul>
<blockquote>
<p><strong>Critical:</strong> Cold items NEVER appear in training data. Their embeddings come purely from multimodal features (inductive inference).</p>
</blockquote>
<h3 id="33-inductive-mode-cold-start-evaluation">3.3 Inductive Mode (Cold-Start Evaluation) </h3>
<p>For Track 3, all models use projection-based cold-start strategies (no ID embeddings):</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Cold-Start Strategy</th>
<th>Key Difference</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LATTICE</strong></td>
<td><code>modal_emb = 0.5 × proj(visual) + 0.5 × proj(text)</code></td>
<td>Linear projection via <code>cold_proj</code></td>
</tr>
<tr>
<td><strong>MICRO</strong></td>
<td><code>modal_emb = 0.5 × proj(visual) + 0.5 × proj(text)</code></td>
<td>Linear projection via <code>image_trs</code>/<code>text_trs</code></td>
</tr>
<tr>
<td><strong>DiffMM</strong></td>
<td><code>modal_emb = 0.5 × proj(visual) + 0.5 × proj(text)</code></td>
<td>LeakyReLU projection via <code>image_trans</code>/<code>text_trans</code></td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!NOTE]<br>
At inference time, all three models use the same projection-based approach for cold items. The performance difference stems from <strong>training dynamics</strong>, not inference-time mechanisms. DiffMM's diffusion training creates more robust modal projections.</p>
</blockquote>
<h3 id="34-dataset-configuration">3.4 Dataset Configuration </h3>
<p><strong>Target Datasets:</strong></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Full Size</th>
<th>Experiment Subset</th>
<th>Sparsity</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Beauty</strong></td>
<td>729K users, 207K items, 6.6M interactions</td>
<td>~12-15K users, ~8-10K items</td>
<td>99.996%</td>
</tr>
<tr>
<td><strong>Electronics</strong></td>
<td>1.6M users, 368K items, 15.5M interactions</td>
<td>~12-15K users, ~8-10K items</td>
<td>99.997%</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note:</strong> Clothing dataset (2.5M users, 715K items) may be included as a stretch goal but requires careful memory management.</p>
</blockquote>
<h3 id="35-model-configuration">3.5 Model Configuration </h3>
<p><strong>Shared Hyperparameters (Fair Comparison):</strong></p>
<p>All models share the following hyperparameters to ensure fair comparison. Values reflect <code>src/common/config.py</code>:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>BATCH_SIZE</code></td>
<td>1024</td>
<td>Gradient noise = implicit regularization</td>
</tr>
<tr>
<td><code>EPOCHS</code></td>
<td>150</td>
<td>With early stopping</td>
</tr>
<tr>
<td><code>PATIENCE</code></td>
<td>25</td>
<td>Early stopping epochs</td>
</tr>
<tr>
<td><code>LR</code></td>
<td>5e-4</td>
<td>Lower LR for deeper models</td>
</tr>
<tr>
<td><code>L2_REG</code></td>
<td>1e-3</td>
<td>Strong weight decay</td>
</tr>
<tr>
<td><code>EMBED_DIM</code></td>
<td>384</td>
<td>Divisible by attention heads (6, 12)</td>
</tr>
<tr>
<td><code>N_LAYERS</code></td>
<td>3</td>
<td>Sweet spot (4 → oversmoothing)</td>
</tr>
<tr>
<td><code>N_NEGATIVES</code></td>
<td>1</td>
<td>Single negative per sample (original paper setting)</td>
</tr>
</tbody>
</table>
<p><strong>Modality Projection (MLP Bridge):</strong></p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>768 (CLIP/SBERT) → 1024 (hidden) → LeakyReLU → Dropout(0.5) → 384 (embed_dim)
</code></pre><table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>PROJECTION_HIDDEN_DIM</code></td>
<td>1024</td>
<td>Non-linear visual→preference mapping</td>
</tr>
<tr>
<td><code>PROJECTION_DROPOUT</code></td>
<td>0.5</td>
<td>Prevent feature memorization</td>
</tr>
</tbody>
</table>
<p><strong>Model-Specific Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Parameter</th>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>LATTICE</td>
<td><code>LATTICE_K</code></td>
<td>10</td>
<td>k-NN neighbors for item graph</td>
</tr>
<tr>
<td>LATTICE</td>
<td><code>LATTICE_LAMBDA</code></td>
<td>0.9</td>
<td>Weight for original vs learned graph (higher = more original)</td>
</tr>
<tr>
<td>LATTICE</td>
<td><code>LATTICE_FEAT_EMBED_DIM</code></td>
<td>64</td>
<td>Modal feature projection dimension</td>
</tr>
<tr>
<td>LATTICE</td>
<td><code>LATTICE_N_ITEM_LAYERS</code></td>
<td>1</td>
<td>Number of item graph conv layers</td>
</tr>
<tr>
<td>MICRO</td>
<td><code>MICRO_TAU</code></td>
<td>0.5</td>
<td>Contrastive temperature</td>
</tr>
<tr>
<td>MICRO</td>
<td><code>MICRO_LOSS_RATIO</code></td>
<td>0.03</td>
<td>Contrastive loss weight</td>
</tr>
<tr>
<td>MICRO</td>
<td><code>MICRO_TOPK</code></td>
<td>10</td>
<td>k-NN neighbors for item graph</td>
</tr>
<tr>
<td>MICRO</td>
<td><code>MICRO_LAMBDA</code></td>
<td>0.9</td>
<td>Weight for original vs learned graph</td>
</tr>
<tr>
<td>DiffMM</td>
<td><code>DIFFMM_STEPS</code></td>
<td>5</td>
<td>Diffusion steps</td>
</tr>
<tr>
<td>DiffMM</td>
<td><code>DIFFMM_NOISE_SCALE</code></td>
<td>0.1</td>
<td>Noise scale factor</td>
</tr>
<tr>
<td>DiffMM</td>
<td><code>DIFFMM_E_LOSS</code></td>
<td>0.1</td>
<td>GraphCL loss weight</td>
</tr>
<tr>
<td>DiffMM</td>
<td><code>DIFFMM_SSL_REG</code></td>
<td>1e-2</td>
<td>Contrastive loss weight (λ_cl)</td>
</tr>
<tr>
<td>DiffMM</td>
<td><code>DIFFMM_TEMP</code></td>
<td>0.5</td>
<td>Contrastive temperature (τ)</td>
</tr>
<tr>
<td>DiffMM</td>
<td><code>DIFFMM_KEEP_RATE</code></td>
<td>0.5</td>
<td>Edge dropout keep rate</td>
</tr>
</tbody>
</table>
<h3 id="36-feature-extraction">3.6 Feature Extraction </h3>
<p><strong>Visual Features (CLIP):</strong></p>
<ul>
<li>Model: <code>openai/clip-vit-large-patch14</code></li>
<li>Output: 768-dimensional embeddings</li>
<li>Anisotropy correction: Mean centering on warm items</li>
</ul>
<p><strong>Text Features (Sentence-BERT):</strong></p>
<ul>
<li>Model: <code>sentence-transformers/all-mpnet-base-v2</code></li>
<li>Input: <code>title + description + features</code> (concatenated)</li>
<li>Output: 768-dimensional embeddings</li>
<li>Anisotropy correction: Mean centering on warm items</li>
</ul>
<p><strong>Preprocessing Pipeline:</strong></p>
<pre data-role="codeBlock" data-info="python" class="language-python python"><code><span class="token comment"># Mean centering (critical for LATTICE/MICRO performance)</span>
x <span class="token operator">=</span> x <span class="token operator">/</span> norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>              <span class="token comment"># L2 normalize</span>
mu <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>n_warm<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># Mean from WARM items only</span>
x <span class="token operator">=</span> x <span class="token operator">-</span> mu                    <span class="token comment"># Center</span>
x <span class="token operator">=</span> x <span class="token operator">/</span> norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>              <span class="token comment"># Re-normalize</span>
</code></pre><h3 id="37-loss-functions">3.7 Loss Functions </h3>
<p><strong>Primary Loss (BPR):</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>L</mi><mtext>BPR</mtext></msub><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>D</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>u</mi><mi>i</mi></mrow></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>u</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{\text{BPR}} = -\sum_{(u,i,j) \in D} \log \sigma(\hat{y}_{ui} - \hat{y}_{uj})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">BPR</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.566em;vertical-align:-1.516em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">u</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>Where:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>u</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(u, i, j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span></span></span></span>: user <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span>, positive item <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>, negative item <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>u</mi><mi>i</mi></mrow></msub><mo>=</mo><msubsup><mi mathvariant="bold">e</mi><mi>u</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi mathvariant="bold">e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y}_{ui} = \mathbf{e}_u^\top \mathbf{e}_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0961em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathbf">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>: predicted score</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>: sigmoid function</li>
</ul>
<p><strong>Auxiliary Losses:</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Auxiliary Loss</th>
<th>Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>LATTICE</td>
<td>L2 regularization only</td>
<td>—</td>
</tr>
<tr>
<td>MICRO</td>
<td>InfoNCE contrastive</td>
<td><code>MICRO_LOSS_RATIO = 0.03</code></td>
</tr>
<tr>
<td>DiffMM</td>
<td>GraphCL + Contrastive</td>
<td><code>DIFFMM_E_LOSS = 0.1</code>, <code>DIFFMM_SSL_REG = 0.01</code></td>
</tr>
</tbody>
</table>
<h3 id="38-training-infrastructure">3.8 Training Infrastructure </h3>
<p><strong>Hardware Configuration:</strong></p>
<ul>
<li>GPU: RTX 3060 (12GB VRAM)</li>
<li>CPU: i5-13500 (6P+8E = 20 threads)</li>
<li>RAM: 64GB</li>
</ul>
<p><strong>Optimizations:</strong></p>
<ul>
<li>Mixed Precision Training (AMP): ~25% VRAM savings, ~15% speedup</li>
<li>Cosine Annealing LR Scheduler</li>
<li>Parallel data loading (6 workers)</li>
<li>Gradient clipping (max_norm=1.0)</li>
</ul>
<hr>
<h2 id="4-experiment-results">4. Experiment Results </h2>
<p>This section presents the experimental results from training LATTICE, MICRO, and DiffMM on three Amazon 2023 datasets (Beauty, Clothing, Electronics).</p>
<blockquote>
<p>Generated tables and figures are saved in <code>experiment_result/</code>.</p>
</blockquote>
<h3 id="41-main-results-track-1-warm-performance">4.1 Main Results (Track 1: Warm Performance) </h3>
<p>The following table reports performance on <strong>warm items</strong> (items seen during training) across all datasets and models. Bold values indicate the best performance per dataset.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Model</th>
<th style="text-align:right">recall@10</th>
<th style="text-align:right">recall@20</th>
<th style="text-align:right">recall@50</th>
<th style="text-align:right">ndcg@10</th>
<th style="text-align:right">ndcg@20</th>
<th style="text-align:right">ndcg@50</th>
<th style="text-align:right">precision@10</th>
<th style="text-align:right">precision@20</th>
<th style="text-align:right">precision@50</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0414</td>
<td style="text-align:right">0.0685</td>
<td style="text-align:right">0.1222</td>
<td style="text-align:right">0.0223</td>
<td style="text-align:right">0.0295</td>
<td style="text-align:right">0.0410</td>
<td style="text-align:right">0.0058</td>
<td style="text-align:right">0.0047</td>
<td style="text-align:right">0.0034</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right"><strong>0.0501</strong></td>
<td style="text-align:right">0.0725</td>
<td style="text-align:right">0.1148</td>
<td style="text-align:right"><strong>0.0307</strong></td>
<td style="text-align:right"><strong>0.0368</strong></td>
<td style="text-align:right"><strong>0.0457</strong></td>
<td style="text-align:right"><strong>0.0068</strong></td>
<td style="text-align:right">0.0049</td>
<td style="text-align:right">0.0032</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0483</td>
<td style="text-align:right"><strong>0.0733</strong></td>
<td style="text-align:right"><strong>0.1202</strong></td>
<td style="text-align:right">0.0283</td>
<td style="text-align:right">0.0349</td>
<td style="text-align:right">0.0449</td>
<td style="text-align:right">0.0067</td>
<td style="text-align:right"><strong>0.0050</strong></td>
<td style="text-align:right"><strong>0.0033</strong></td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0233</td>
<td style="text-align:right">0.0413</td>
<td style="text-align:right"><strong>0.0819</strong></td>
<td style="text-align:right">0.0126</td>
<td style="text-align:right">0.0174</td>
<td style="text-align:right">0.0259</td>
<td style="text-align:right">0.0031</td>
<td style="text-align:right">0.0027</td>
<td style="text-align:right"><strong>0.0022</strong></td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left"><strong>LATTICE</strong></td>
<td style="text-align:right"><strong>0.0333</strong></td>
<td style="text-align:right"><strong>0.0468</strong></td>
<td style="text-align:right">0.0708</td>
<td style="text-align:right"><strong>0.0182</strong></td>
<td style="text-align:right"><strong>0.0218</strong></td>
<td style="text-align:right">0.0269</td>
<td style="text-align:right"><strong>0.0044</strong></td>
<td style="text-align:right"><strong>0.0031</strong></td>
<td style="text-align:right">0.0019</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0313</td>
<td style="text-align:right">0.0461</td>
<td style="text-align:right">0.0787</td>
<td style="text-align:right">0.0170</td>
<td style="text-align:right">0.0210</td>
<td style="text-align:right"><strong>0.0278</strong></td>
<td style="text-align:right">0.0042</td>
<td style="text-align:right"><strong>0.0031</strong></td>
<td style="text-align:right">0.0021</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0519</td>
<td style="text-align:right">0.0819</td>
<td style="text-align:right"><strong>0.1388</strong></td>
<td style="text-align:right">0.0299</td>
<td style="text-align:right">0.0379</td>
<td style="text-align:right">0.0501</td>
<td style="text-align:right">0.0073</td>
<td style="text-align:right">0.0057</td>
<td style="text-align:right"><strong>0.0038</strong></td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0579</td>
<td style="text-align:right">0.0855</td>
<td style="text-align:right">0.1376</td>
<td style="text-align:right">0.0325</td>
<td style="text-align:right">0.0400</td>
<td style="text-align:right">0.0511</td>
<td style="text-align:right">0.0080</td>
<td style="text-align:right">0.0059</td>
<td style="text-align:right"><strong>0.0038</strong></td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left"><strong>MICRO</strong></td>
<td style="text-align:right"><strong>0.0622</strong></td>
<td style="text-align:right"><strong>0.0864</strong></td>
<td style="text-align:right">0.1377</td>
<td style="text-align:right"><strong>0.0353</strong></td>
<td style="text-align:right"><strong>0.0419</strong></td>
<td style="text-align:right"><strong>0.0528</strong></td>
<td style="text-align:right"><strong>0.0087</strong></td>
<td style="text-align:right"><strong>0.0061</strong></td>
<td style="text-align:right"><strong>0.0038</strong></td>
</tr>
</tbody>
</table>
<p><strong>Key Findings (Track 1 - Warm Performance):</strong></p>
<ol>
<li>
<p><strong>MICRO achieves best overall Recall@20</strong> across all three datasets:</p>
<ul>
<li>Beauty: 0.0733 (LATTICE close at 0.0725)</li>
<li>Clothing: 0.0461 (LATTICE leads at 0.0468, but MICRO has better Recall@50)</li>
<li>Electronics: <strong>0.0864</strong> (clear lead over LATTICE 0.0855 and DiffMM 0.0819)</li>
</ul>
</li>
<li>
<p><strong>LATTICE shows strong NDCG performance</strong> on Beauty (0.0368) and Clothing (0.0218), indicating better ranking quality despite slightly lower recall on some datasets.</p>
</li>
<li>
<p><strong>DiffMM underperforms on warm items</strong> but shows competitive Recall@50, suggesting it may retrieve relevant items deeper in the ranking.</p>
</li>
</ol>
<h3 id="42-user-robustness-track-2-sparse-vs-active-users">4.2 User Robustness (Track 2: Sparse vs. Active Users) </h3>
<p>This table compares model performance across user activity levels to assess <strong>user robustness</strong>—whether models disproportionately favor users with rich interaction history.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Model</th>
<th style="text-align:left">User Type</th>
<th style="text-align:right">recall@10</th>
<th style="text-align:right">recall@20</th>
<th style="text-align:right">recall@50</th>
<th style="text-align:right">ndcg@10</th>
<th style="text-align:right">ndcg@20</th>
<th style="text-align:right">ndcg@50</th>
<th style="text-align:right">precision@10</th>
<th style="text-align:right">precision@20</th>
<th style="text-align:right">precision@50</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0557</td>
<td style="text-align:right">0.0642</td>
<td style="text-align:right">0.1072</td>
<td style="text-align:right">0.0419</td>
<td style="text-align:right">0.0458</td>
<td style="text-align:right">0.0579</td>
<td style="text-align:right">0.0150</td>
<td style="text-align:right">0.0098</td>
<td style="text-align:right">0.0069</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0377</td>
<td style="text-align:right">0.0648</td>
<td style="text-align:right">0.1172</td>
<td style="text-align:right">0.0192</td>
<td style="text-align:right">0.0264</td>
<td style="text-align:right">0.0374</td>
<td style="text-align:right">0.0047</td>
<td style="text-align:right">0.0040</td>
<td style="text-align:right">0.0030</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0462</td>
<td style="text-align:right">0.0906</td>
<td style="text-align:right">0.1336</td>
<td style="text-align:right">0.0443</td>
<td style="text-align:right">0.0579</td>
<td style="text-align:right">0.0689</td>
<td style="text-align:right">0.0187</td>
<td style="text-align:right">0.0140</td>
<td style="text-align:right">0.0080</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0485</td>
<td style="text-align:right">0.0677</td>
<td style="text-align:right">0.1042</td>
<td style="text-align:right">0.0299</td>
<td style="text-align:right">0.0350</td>
<td style="text-align:right">0.0426</td>
<td style="text-align:right">0.0060</td>
<td style="text-align:right">0.0042</td>
<td style="text-align:right">0.0026</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0532</td>
<td style="text-align:right">0.0749</td>
<td style="text-align:right">0.1246</td>
<td style="text-align:right">0.0446</td>
<td style="text-align:right">0.0530</td>
<td style="text-align:right">0.0669</td>
<td style="text-align:right">0.0178</td>
<td style="text-align:right">0.0136</td>
<td style="text-align:right">0.0088</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0462</td>
<td style="text-align:right">0.0697</td>
<td style="text-align:right">0.1130</td>
<td style="text-align:right">0.0266</td>
<td style="text-align:right">0.0328</td>
<td style="text-align:right">0.0419</td>
<td style="text-align:right">0.0057</td>
<td style="text-align:right">0.0043</td>
<td style="text-align:right">0.0028</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0265</td>
<td style="text-align:right">0.0324</td>
<td style="text-align:right">0.0582</td>
<td style="text-align:right">0.0178</td>
<td style="text-align:right">0.0201</td>
<td style="text-align:right">0.0292</td>
<td style="text-align:right">0.0088</td>
<td style="text-align:right">0.0059</td>
<td style="text-align:right">0.0053</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0223</td>
<td style="text-align:right">0.0411</td>
<td style="text-align:right">0.0786</td>
<td style="text-align:right">0.0120</td>
<td style="text-align:right">0.0169</td>
<td style="text-align:right">0.0247</td>
<td style="text-align:right">0.0028</td>
<td style="text-align:right">0.0026</td>
<td style="text-align:right">0.0020</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0535</td>
<td style="text-align:right">0.0670</td>
<td style="text-align:right">0.0896</td>
<td style="text-align:right">0.0336</td>
<td style="text-align:right">0.0400</td>
<td style="text-align:right">0.0466</td>
<td style="text-align:right">0.0176</td>
<td style="text-align:right">0.0132</td>
<td style="text-align:right">0.0071</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0352</td>
<td style="text-align:right">0.0491</td>
<td style="text-align:right">0.0721</td>
<td style="text-align:right">0.0198</td>
<td style="text-align:right">0.0235</td>
<td style="text-align:right">0.0283</td>
<td style="text-align:right">0.0045</td>
<td style="text-align:right">0.0031</td>
<td style="text-align:right">0.0018</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0268</td>
<td style="text-align:right">0.0670</td>
<td style="text-align:right">0.1209</td>
<td style="text-align:right">0.0205</td>
<td style="text-align:right">0.0343</td>
<td style="text-align:right">0.0471</td>
<td style="text-align:right">0.0118</td>
<td style="text-align:right">0.0118</td>
<td style="text-align:right">0.0071</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0339</td>
<td style="text-align:right">0.0493</td>
<td style="text-align:right">0.0785</td>
<td style="text-align:right">0.0184</td>
<td style="text-align:right">0.0225</td>
<td style="text-align:right">0.0286</td>
<td style="text-align:right">0.0043</td>
<td style="text-align:right">0.0031</td>
<td style="text-align:right">0.0020</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0502</td>
<td style="text-align:right">0.0941</td>
<td style="text-align:right">0.1469</td>
<td style="text-align:right">0.0375</td>
<td style="text-align:right">0.0524</td>
<td style="text-align:right">0.0677</td>
<td style="text-align:right">0.0176</td>
<td style="text-align:right">0.0154</td>
<td style="text-align:right">0.0101</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0543</td>
<td style="text-align:right">0.0864</td>
<td style="text-align:right">0.1422</td>
<td style="text-align:right">0.0301</td>
<td style="text-align:right">0.0385</td>
<td style="text-align:right">0.0503</td>
<td style="text-align:right">0.0068</td>
<td style="text-align:right">0.0053</td>
<td style="text-align:right">0.0036</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0668</td>
<td style="text-align:right">0.1071</td>
<td style="text-align:right">0.1500</td>
<td style="text-align:right">0.0421</td>
<td style="text-align:right">0.0580</td>
<td style="text-align:right">0.0703</td>
<td style="text-align:right">0.0198</td>
<td style="text-align:right">0.0181</td>
<td style="text-align:right">0.0103</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0588</td>
<td style="text-align:right">0.0872</td>
<td style="text-align:right">0.1397</td>
<td style="text-align:right">0.0327</td>
<td style="text-align:right">0.0402</td>
<td style="text-align:right">0.0511</td>
<td style="text-align:right">0.0074</td>
<td style="text-align:right">0.0054</td>
<td style="text-align:right">0.0035</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:left">Active (≥20)</td>
<td style="text-align:right">0.0716</td>
<td style="text-align:right">0.1081</td>
<td style="text-align:right">0.1485</td>
<td style="text-align:right">0.0453</td>
<td style="text-align:right">0.0591</td>
<td style="text-align:right">0.0712</td>
<td style="text-align:right">0.0242</td>
<td style="text-align:right">0.0192</td>
<td style="text-align:right">0.0105</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:left">Sparse (≤5)</td>
<td style="text-align:right">0.0629</td>
<td style="text-align:right">0.0874</td>
<td style="text-align:right">0.1371</td>
<td style="text-align:right">0.0357</td>
<td style="text-align:right">0.0421</td>
<td style="text-align:right">0.0525</td>
<td style="text-align:right">0.0080</td>
<td style="text-align:right">0.0055</td>
<td style="text-align:right">0.0035</td>
</tr>
</tbody>
</table>
<h4 id="activesparse-performance-ratio-recall20">Active/Sparse Performance Ratio (Recall@20) </h4>
<p>To quantify the performance gap, we compute the <strong>Active/Sparse Ratio</strong> = <code>Recall@20(Active) / Recall@20(Sparse)</code>. A ratio closer to 1.0 indicates better user fairness:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Model</th>
<th style="text-align:right">Sparse R@20</th>
<th style="text-align:right">Active R@20</th>
<th style="text-align:right">Active/Sparse Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0648</td>
<td style="text-align:right">0.0642</td>
<td style="text-align:right"><strong>0.99×</strong> (fair)</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0677</td>
<td style="text-align:right">0.0906</td>
<td style="text-align:right"><strong>1.34×</strong></td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0697</td>
<td style="text-align:right">0.0749</td>
<td style="text-align:right"><strong>1.07×</strong></td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0411</td>
<td style="text-align:right">0.0324</td>
<td style="text-align:right"><strong>0.79×</strong> (sparse wins!)</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0491</td>
<td style="text-align:right">0.0670</td>
<td style="text-align:right"><strong>1.36×</strong></td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0493</td>
<td style="text-align:right">0.0670</td>
<td style="text-align:right"><strong>1.36×</strong></td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0864</td>
<td style="text-align:right">0.0941</td>
<td style="text-align:right"><strong>1.09×</strong></td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0872</td>
<td style="text-align:right">0.1071</td>
<td style="text-align:right"><strong>1.23×</strong></td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0874</td>
<td style="text-align:right">0.1081</td>
<td style="text-align:right"><strong>1.24×</strong></td>
</tr>
</tbody>
</table>
<p><strong>Key Findings (Track 2 - User Robustness):</strong></p>
<ol>
<li>
<p><strong>DiffMM shows remarkable user fairness</strong> (ratios near or below 1.0×):</p>
<ul>
<li>Beauty: 0.99× — virtually identical performance for sparse vs. active users.</li>
<li>Clothing: <strong>0.79×</strong> — DiffMM actually performs <em>better</em> on sparse users than active users!</li>
<li>This confirms DiffMM's generative augmentation effectively compensates for sparse user histories.</li>
</ul>
</li>
<li>
<p><strong>LATTICE and MICRO favor active users</strong> (ratios 1.07-1.36×):</p>
<ul>
<li>Both structure-learning methods show consistent bias toward users with richer interaction graphs.</li>
<li>LATTICE exhibits the largest disparity on Beauty (1.34×), indicating its k-NN graph construction relies heavily on collaborative density.</li>
</ul>
</li>
<li>
<p><strong>Electronics maintains moderate gaps</strong> (1.09-1.24×):</p>
<ul>
<li>All models show smaller active/sparse gaps compared to Beauty/Clothing.</li>
<li>Functional product features provide stronger content-based signals that partially compensate for sparse history.</li>
</ul>
</li>
<li>
<p><strong>Precision gap remains significant:</strong></p>
<ul>
<li>Active users consistently achieve 2-3× higher precision (e.g., Electronics MICRO: 0.0192 vs 0.0055).</li>
</ul>
</li>
</ol>
<h3 id="43-cold-start-performance-track-3-inductive-evaluation">4.3 Cold-Start Performance (Track 3: Inductive Evaluation) </h3>
<p>This section evaluates model performance on <strong>cold items</strong>—items that never appeared during training. This is the most challenging evaluation track, as models must represent items using only their multimodal features (CLIP visual embeddings, SBERT text embeddings) without any ID-based learned representations.</p>
<blockquote>
<p>[!IMPORTANT]<br>
<strong>Inductive Inference:</strong> For cold items, all models use the same projection-based strategy:</p>
<ul>
<li><code>item_emb = 0.5 × proj(visual) + 0.5 × proj(text)</code> — no ID embedding, only modal features</li>
<li>The key difference is how <strong>training dynamics</strong> shape the projection quality (see interpretation below)</li>
</ul>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Model</th>
<th style="text-align:right">recall@10</th>
<th style="text-align:right">recall@20</th>
<th style="text-align:right">recall@50</th>
<th style="text-align:right">ndcg@10</th>
<th style="text-align:right">ndcg@20</th>
<th style="text-align:right">ndcg@50</th>
<th style="text-align:right">precision@10</th>
<th style="text-align:right">precision@20</th>
<th style="text-align:right">precision@50</th>
<th style="text-align:right">Cold/Warm@10</th>
<th style="text-align:right">Cold/Warm@20</th>
<th style="text-align:right">Cold/Warm@50</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left"><strong>DiffMM</strong></td>
<td style="text-align:right"><strong>0.0491</strong></td>
<td style="text-align:right"><strong>0.0827</strong></td>
<td style="text-align:right"><strong>0.1474</strong></td>
<td style="text-align:right"><strong>0.0287</strong></td>
<td style="text-align:right"><strong>0.0385</strong></td>
<td style="text-align:right"><strong>0.0537</strong></td>
<td style="text-align:right"><strong>0.0091</strong></td>
<td style="text-align:right"><strong>0.0077</strong></td>
<td style="text-align:right"><strong>0.0056</strong></td>
<td style="text-align:right"><strong>1.1865</strong></td>
<td style="text-align:right"><strong>1.2075</strong></td>
<td style="text-align:right"><strong>1.2061</strong></td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0061</td>
<td style="text-align:right">0.0132</td>
<td style="text-align:right">0.0337</td>
<td style="text-align:right">0.0033</td>
<td style="text-align:right">0.0054</td>
<td style="text-align:right">0.0102</td>
<td style="text-align:right">0.0013</td>
<td style="text-align:right">0.0013</td>
<td style="text-align:right">0.0014</td>
<td style="text-align:right">0.1226</td>
<td style="text-align:right">0.1825</td>
<td style="text-align:right">0.2935</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0054</td>
<td style="text-align:right">0.0122</td>
<td style="text-align:right">0.0301</td>
<td style="text-align:right">0.0027</td>
<td style="text-align:right">0.0047</td>
<td style="text-align:right">0.0088</td>
<td style="text-align:right">0.0010</td>
<td style="text-align:right">0.0011</td>
<td style="text-align:right">0.0011</td>
<td style="text-align:right">0.1114</td>
<td style="text-align:right">0.1666</td>
<td style="text-align:right">0.2506</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left"><strong>DiffMM</strong></td>
<td style="text-align:right"><strong>0.0390</strong></td>
<td style="text-align:right"><strong>0.0659</strong></td>
<td style="text-align:right"><strong>0.1211</strong></td>
<td style="text-align:right"><strong>0.0240</strong></td>
<td style="text-align:right"><strong>0.0320</strong></td>
<td style="text-align:right"><strong>0.0449</strong></td>
<td style="text-align:right"><strong>0.0076</strong></td>
<td style="text-align:right"><strong>0.0064</strong></td>
<td style="text-align:right"><strong>0.0047</strong></td>
<td style="text-align:right"><strong>1.6759</strong></td>
<td style="text-align:right"><strong>1.5956</strong></td>
<td style="text-align:right"><strong>1.4793</strong></td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0050</td>
<td style="text-align:right">0.0094</td>
<td style="text-align:right">0.0242</td>
<td style="text-align:right">0.0028</td>
<td style="text-align:right">0.0042</td>
<td style="text-align:right">0.0077</td>
<td style="text-align:right">0.0010</td>
<td style="text-align:right">0.0010</td>
<td style="text-align:right">0.0010</td>
<td style="text-align:right">0.1513</td>
<td style="text-align:right">0.2009</td>
<td style="text-align:right">0.3411</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0042</td>
<td style="text-align:right">0.0091</td>
<td style="text-align:right">0.0225</td>
<td style="text-align:right">0.0022</td>
<td style="text-align:right">0.0036</td>
<td style="text-align:right">0.0067</td>
<td style="text-align:right">0.0008</td>
<td style="text-align:right">0.0009</td>
<td style="text-align:right">0.0008</td>
<td style="text-align:right">0.1345</td>
<td style="text-align:right">0.1977</td>
<td style="text-align:right">0.2861</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left"><strong>DiffMM</strong></td>
<td style="text-align:right"><strong>0.0333</strong></td>
<td style="text-align:right"><strong>0.0551</strong></td>
<td style="text-align:right"><strong>0.1037</strong></td>
<td style="text-align:right"><strong>0.0192</strong></td>
<td style="text-align:right"><strong>0.0258</strong></td>
<td style="text-align:right"><strong>0.0374</strong></td>
<td style="text-align:right"><strong>0.0067</strong></td>
<td style="text-align:right"><strong>0.0056</strong></td>
<td style="text-align:right"><strong>0.0043</strong></td>
<td style="text-align:right"><strong>0.6414</strong></td>
<td style="text-align:right"><strong>0.6729</strong></td>
<td style="text-align:right"><strong>0.7473</strong></td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0072</td>
<td style="text-align:right">0.0143</td>
<td style="text-align:right">0.0307</td>
<td style="text-align:right">0.0042</td>
<td style="text-align:right">0.0063</td>
<td style="text-align:right">0.0102</td>
<td style="text-align:right">0.0016</td>
<td style="text-align:right">0.0015</td>
<td style="text-align:right">0.0013</td>
<td style="text-align:right">0.1243</td>
<td style="text-align:right">0.1667</td>
<td style="text-align:right">0.2232</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0053</td>
<td style="text-align:right">0.0101</td>
<td style="text-align:right">0.0288</td>
<td style="text-align:right">0.0029</td>
<td style="text-align:right">0.0043</td>
<td style="text-align:right">0.0088</td>
<td style="text-align:right">0.0012</td>
<td style="text-align:right">0.0011</td>
<td style="text-align:right">0.0012</td>
<td style="text-align:right">0.0857</td>
<td style="text-align:right">0.1167</td>
<td style="text-align:right">0.2091</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Cold/Warm Ratio</strong> = <code>Cold_Recall@K / Warm_Recall@K</code>. Values &gt;1.0 indicate the model performs <em>better</em> on cold items than warm items.</p>
</blockquote>
<p><strong>Key Findings (Track 3 - Cold-Start):</strong></p>
<blockquote>
<p>[!CAUTION]<br>
<strong>Critical Finding:</strong> The cold-start results reveal a dramatic architectural divergence. DiffMM's generative approach excels at inductive inference, while LATTICE and MICRO's deterministic projections fail catastrophically.</p>
</blockquote>
<ol>
<li>
<p><strong>DiffMM excels at cold-start</strong> with Cold/Warm ratios exceeding 100%:</p>
<ul>
<li><strong>Clothing:</strong> 159.6% Cold/Warm@20 — DiffMM performs <strong>60% better</strong> on unseen items than warm items!</li>
<li><strong>Beauty:</strong> 120.8% Cold/Warm@20 — consistent improvement on cold items.</li>
<li><strong>Electronics:</strong> 67.3% Cold/Warm@20 — more challenging, but still far ahead of alternatives.</li>
</ul>
<p><strong>Interpretation:</strong> Although all models use the same projection-based inference, DiffMM's superior cold-start performance stems from its <strong>training dynamics</strong>: (1) the diffusion objective forces modal projections to capture reconstructable semantics, (2) the cross-modal contrastive loss aligns visual/text spaces better, and (3) LeakyReLU activations in projections provide richer non-linear mappings compared to LATTICE/MICRO's linear projections.</p>
</li>
<li>
<p><strong>LATTICE and MICRO fail catastrophically</strong> with Cold/Warm ratios of 11-34%:</p>
<ul>
<li><strong>Beauty LATTICE:</strong> 18.3% Cold/Warm@20 — an <strong>82% performance drop</strong> on cold items.</li>
<li><strong>Beauty MICRO:</strong> 16.7% Cold/Warm@20 — even worse than LATTICE.</li>
<li><strong>Electronics MICRO:</strong> 11.7% Cold/Warm@20 — the worst cold-start performance.</li>
</ul>
<p><strong>Interpretation:</strong> LATTICE/MICRO's linear projections (without non-linear activations) and training objectives (BPR + optional contrastive) do not explicitly encourage generalizable modal representations. Their projections learn ID-dependent patterns during training that fail to transfer to unseen items.</p>
</li>
<li>
<p><strong>Domain effects persist but are secondary:</strong></p>
<ul>
<li>Clothing shows the strongest cold-start performance for DiffMM (159.6%), likely because visual/text features provide strong inductive signals for fashion items.</li>
<li>Electronics is hardest for all models, suggesting functional products rely more on collaborative signals that are unavailable for cold items.</li>
</ul>
</li>
</ol>
<h3 id="44-training-dynamics">4.4 Training Dynamics </h3>
<p>The following figures show training loss (left) and validation Recall@20 (right) across epochs, enabling comparison of loss convergence with generalization performance.</p>
<p><strong>Beauty:</strong></p>
<p><img src="../experiment_result/figures/training_combined_beauty.png" alt="Beauty Training Dynamics"></p>
<p><strong>Clothing:</strong></p>
<p><img src="../experiment_result/figures/training_combined_clothing.png" alt="Clothing Training Dynamics"></p>
<p><strong>Electronics:</strong></p>
<p><img src="../experiment_result/figures/training_combined_electronics.png" alt="Electronics Training Dynamics"></p>
<p><strong>Key Observations:</strong></p>
<ol>
<li><strong>Convergence Speed:</strong> DiffMM converges faster (steeper loss decrease in early epochs) but plateaus earlier than LATTICE/MICRO.</li>
<li><strong>Validation Stability:</strong> MICRO shows the smoothest validation curves, while LATTICE exhibits more oscillation—likely due to k-NN graph updates during training.</li>
<li><strong>Overfitting Indicators:</strong> DiffMM's validation Recall@20 peaks earlier (~epoch 50-100) then shows slight decline, suggesting earlier early-stopping would benefit this model.</li>
<li><strong>Electronics Advantage:</strong> All models achieve higher absolute Recall@20 on Electronics, consistent with the strong multimodal alignment observed in EDA.</li>
</ol>
<hr>
<h2 id="5-in-depth-analysis">5. In-Depth Analysis </h2>
<h3 id="51-ablation-studies">5.1 Ablation Studies </h3>
<p>To answer <strong>RQ1</strong> (modality contribution per domain), we conduct three-way ablation by removing each modality:</p>
<table>
<thead>
<tr>
<th>Condition</th>
<th>Visual Features</th>
<th>Text Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Full</strong></td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td><strong>No-Visual</strong></td>
<td>Zeroed</td>
<td>✓</td>
</tr>
<tr>
<td><strong>No-Text</strong></td>
<td>✓</td>
<td>Zeroed</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Generated tables and figures are saved in <code>ablation_result/</code>.</p>
</blockquote>
<h4 id="511-modality-contribution-track-1-warm-performance">5.1.1 Modality Contribution (Track 1: Warm Performance) </h4>
<p>The following table shows the <strong>percentage drop</strong> in Recall@20 when each modality is removed. Larger drops indicate higher modality importance:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Model</th>
<th style="text-align:right">Full R@20</th>
<th style="text-align:right">No-Visual R@20</th>
<th style="text-align:right">No-Text R@20</th>
<th style="text-align:right">Visual Drop (%)</th>
<th style="text-align:right">Text Drop (%)</th>
<th style="text-align:left">Dominant</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0725</td>
<td style="text-align:right">0.0706</td>
<td style="text-align:right">0.0710</td>
<td style="text-align:right">2.61</td>
<td style="text-align:right">2.06</td>
<td style="text-align:left">Visual</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0733</td>
<td style="text-align:right">0.0670</td>
<td style="text-align:right">0.0684</td>
<td style="text-align:right">8.54</td>
<td style="text-align:right">6.64</td>
<td style="text-align:left">Visual</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0685</td>
<td style="text-align:right">0.0645</td>
<td style="text-align:right">0.0633</td>
<td style="text-align:right">5.82</td>
<td style="text-align:right">7.60</td>
<td style="text-align:left">Text</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0468</td>
<td style="text-align:right">0.0314</td>
<td style="text-align:right">0.0303</td>
<td style="text-align:right"><strong>32.92</strong></td>
<td style="text-align:right"><strong>35.24</strong></td>
<td style="text-align:left">Text</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0461</td>
<td style="text-align:right">0.0462</td>
<td style="text-align:right">0.0338</td>
<td style="text-align:right">-0.16</td>
<td style="text-align:right"><strong>26.76</strong></td>
<td style="text-align:left">Text</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0413</td>
<td style="text-align:right">0.0387</td>
<td style="text-align:right">0.0403</td>
<td style="text-align:right">6.41</td>
<td style="text-align:right">2.34</td>
<td style="text-align:left">Visual</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0855</td>
<td style="text-align:right">0.0798</td>
<td style="text-align:right">0.0802</td>
<td style="text-align:right">6.73</td>
<td style="text-align:right">6.25</td>
<td style="text-align:left">Visual</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0864</td>
<td style="text-align:right">0.0829</td>
<td style="text-align:right">0.0806</td>
<td style="text-align:right">4.09</td>
<td style="text-align:right">6.69</td>
<td style="text-align:left">Text</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">DiffMM</td>
<td style="text-align:right">0.0819</td>
<td style="text-align:right">0.0825</td>
<td style="text-align:right">0.0802</td>
<td style="text-align:right">-0.82</td>
<td style="text-align:right">1.99</td>
<td style="text-align:left">Text</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Drop (%)</strong> = (Full - Ablated) / Full × 100. <strong>Negative values</strong> indicate performance <em>improved</em> when modality was removed.</p>
</blockquote>
<p><strong>Key Findings (Warm):</strong></p>
<ol>
<li>
<p><strong>Clothing shows extreme modality sensitivity</strong> (26-35% drops):</p>
<ul>
<li>LATTICE: Both modalities critical — 32.9% visual drop, 35.2% text drop.</li>
<li>MICRO: Text-dominant with 26.8% drop when text removed, but removing visual has no effect (-0.16%).</li>
<li>This indicates Clothing recommendations rely heavily on rich textual descriptions (brand, style, material).</li>
</ul>
</li>
<li>
<p><strong>Beauty is model-dependent but balanced:</strong></p>
<ul>
<li>LATTICE/MICRO favor visual (2.6-8.5% drop).</li>
<li>DiffMM favors text (7.6% drop) — its generative sampling may capture textual semantics more effectively.</li>
</ul>
</li>
<li>
<p><strong>Electronics shows lowest modality sensitivity</strong> (0.8-6.7% drops):</p>
<ul>
<li>All models perform relatively well even with one modality removed.</li>
<li>DiffMM actually improves slightly when visual is removed (-0.82%), confirming visual features can be noise for functional products.</li>
</ul>
</li>
</ol>
<h4 id="512-modality-contribution-track-3-cold-start">5.1.2 Modality Contribution (Track 3: Cold-Start) </h4>
<p>Since LATTICE and MICRO fail catastrophically on cold-start (see Section 4.3), the ablation analysis for cold items focuses on <strong>DiffMM</strong> — the only model with viable cold-start performance. We also include LATTICE/MICRO for completeness:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Model</th>
<th style="text-align:right">Full R@20</th>
<th style="text-align:right">No-Visual R@20</th>
<th style="text-align:right">No-Text R@20</th>
<th style="text-align:right">Visual Drop (%)</th>
<th style="text-align:right">Text Drop (%)</th>
<th style="text-align:left">Dominant</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0132</td>
<td style="text-align:right">0.0124</td>
<td style="text-align:right">0.0135</td>
<td style="text-align:right">5.95</td>
<td style="text-align:right">-2.01</td>
<td style="text-align:left">Visual</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0122</td>
<td style="text-align:right">0.0125</td>
<td style="text-align:right">0.0137</td>
<td style="text-align:right">-2.25</td>
<td style="text-align:right">-12.29</td>
<td style="text-align:left">Neither</td>
</tr>
<tr>
<td style="text-align:left">Beauty</td>
<td style="text-align:left"><strong>DiffMM</strong></td>
<td style="text-align:right"><strong>0.0827</strong></td>
<td style="text-align:right">0.0769</td>
<td style="text-align:right">0.0730</td>
<td style="text-align:right"><strong>7.09</strong></td>
<td style="text-align:right"><strong>11.79</strong></td>
<td style="text-align:left"><strong>Text</strong></td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0094</td>
<td style="text-align:right">0.0108</td>
<td style="text-align:right">0.0119</td>
<td style="text-align:right">-14.93</td>
<td style="text-align:right">-26.40</td>
<td style="text-align:left">Neither</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0091</td>
<td style="text-align:right">0.0090</td>
<td style="text-align:right">0.0108</td>
<td style="text-align:right">1.37</td>
<td style="text-align:right">-18.40</td>
<td style="text-align:left">Neither</td>
</tr>
<tr>
<td style="text-align:left">Clothing</td>
<td style="text-align:left"><strong>DiffMM</strong></td>
<td style="text-align:right"><strong>0.0659</strong></td>
<td style="text-align:right">0.0623</td>
<td style="text-align:right">0.0543</td>
<td style="text-align:right"><strong>5.46</strong></td>
<td style="text-align:right"><strong>17.54</strong></td>
<td style="text-align:left"><strong>Text</strong></td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">LATTICE</td>
<td style="text-align:right">0.0143</td>
<td style="text-align:right">0.0115</td>
<td style="text-align:right">0.0103</td>
<td style="text-align:right">18.97</td>
<td style="text-align:right">27.45</td>
<td style="text-align:left">Text</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left">MICRO</td>
<td style="text-align:right">0.0101</td>
<td style="text-align:right">0.0097</td>
<td style="text-align:right">0.0124</td>
<td style="text-align:right">3.50</td>
<td style="text-align:right">-22.97</td>
<td style="text-align:left">Neither</td>
</tr>
<tr>
<td style="text-align:left">Electronics</td>
<td style="text-align:left"><strong>DiffMM</strong></td>
<td style="text-align:right"><strong>0.0551</strong></td>
<td style="text-align:right">0.0715</td>
<td style="text-align:right">0.0665</td>
<td style="text-align:right"><strong>-29.87</strong></td>
<td style="text-align:right"><strong>-20.72</strong></td>
<td style="text-align:left"><strong>Neither</strong></td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!NOTE]<br>
For LATTICE/MICRO, the Full R@20 values are extremely low (0.01) because these models fail at cold-start (see Track 3 results). The ablation % changes are therefore unreliable for these models.</p>
</blockquote>
<p><strong>Key Findings (Cold-Start Ablation — DiffMM Only):</strong></p>
<ol>
<li>
<p><strong>Text is critical for DiffMM cold-start</strong> on aesthetic domains:</p>
<ul>
<li>Beauty: 11.8% drop when text removed.</li>
<li>Clothing: 17.5% drop when text removed.</li>
<li>Product descriptions provide essential semantic grounding for unseen items.</li>
</ul>
</li>
<li>
<p><strong>Electronics shows counter-intuitive results:</strong></p>
<ul>
<li>DiffMM actually <strong>improves</strong> when either modality is removed (-20% to -30%).</li>
<li>This suggests that for functional products, DiffMM's diffusion sampling may benefit from simpler feature inputs, avoiding modality conflicts.</li>
</ul>
</li>
<li>
<p><strong>LATTICE/MICRO ablation results are unreliable:</strong></p>
<ul>
<li>Both models perform near-random on cold items (R@20 ≈ 0.01).</li>
<li>Negative drops (improvements when modality removed) indicate the base Full condition is already at failure level.</li>
</ul>
</li>
</ol>
<h4 id="513-training-dynamics-overview">5.1.3 Training Dynamics (Overview) </h4>
<p>The following figures show all 9 ablation conditions (3 models × 3 conditions) per dataset:</p>
<p><img src="../ablation_result/figures/ablation_overview_beauty.png" alt="Beauty Ablation Overview"></p>
<p><img src="../ablation_result/figures/ablation_overview_clothing.png" alt="Clothing Ablation Overview"></p>
<p><img src="../ablation_result/figures/ablation_overview_electronics.png" alt="Electronics Ablation Overview"></p>
<p><strong>Observations:</strong></p>
<ul>
<li>Full multimodal (solid lines) generally achieves highest validation recall</li>
<li>No-Visual (dashed) and No-Text (dotted) cluster below full, with gap size reflecting modality importance</li>
<li>Electronics shows ablation curves <em>above</em> full for DiffMM/MICRO, confirming visual noise hypothesis</li>
</ul>
<h4 id="514-per-model-ablation-analysis">5.1.4 Per-Model Ablation Analysis </h4>
<p><strong>LATTICE:</strong></p>
<p><img src="../ablation_result/figures/ablation_lattice_beauty.png" alt="LATTICE Beauty"></p>
<p><img src="../ablation_result/figures/ablation_lattice_clothing.png" alt="LATTICE Clothing"></p>
<p><img src="../ablation_result/figures/ablation_lattice_electronics.png" alt="LATTICE Electronics"></p>
<p><strong>MICRO:</strong></p>
<p><img src="../ablation_result/figures/ablation_micro_beauty.png" alt="MICRO Beauty"></p>
<p><img src="../ablation_result/figures/ablation_micro_clothing.png" alt="MICRO Clothing"></p>
<p><img src="../ablation_result/figures/ablation_micro_electronics.png" alt="MICRO Electronics"></p>
<p><strong>DiffMM:</strong></p>
<p><img src="../ablation_result/figures/ablation_diffmm_beauty.png" alt="DiffMM Beauty"></p>
<p><img src="../ablation_result/figures/ablation_diffmm_clothing.png" alt="DiffMM Clothing"></p>
<p><img src="../ablation_result/figures/ablation_diffmm_electronics.png" alt="DiffMM Electronics"></p>
<h3 id="52-sensitivity-analysis">5.2 Sensitivity Analysis </h3>
<blockquote>
<p>Sensitivity analysis results to be populated from <code>scripts/run_sensitivity.py</code> outputs.</p>
</blockquote>
<hr>
<h2 id="6-conclusion">6. Conclusion </h2>
<p>This section synthesizes our experimental findings to address the formalized research questions posed in Section 1.</p>
<h3 id="61-addressing-primary-research-questions">6.1 Addressing Primary Research Questions </h3>
<h4 id="rq1-modality-sensitivity">RQ1: Modality Sensitivity </h4>
<p><strong>Hypothesis:</strong> We hypothesize that the visual modality exhibits a stronger inductive bias in aesthetic-centric domains (e.g., Clothing, Beauty), whereas the textual modality provides superior disambiguation in functional domains (e.g., Electronics).</p>
<p><strong>Validation Method:</strong> Component-Level Ablation (ΔNDCG@20 by masking <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> independently)</p>
<p><strong>Empirical Findings:</strong></p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Visual Ablation (ΔRecall@20)</th>
<th>Text Ablation (ΔRecall@20)</th>
<th>Dominant Modality</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Clothing</strong></td>
<td>-0.2% to -32.9%</td>
<td><strong>-26.8% to -35.2%</strong></td>
<td><strong>Text</strong> (unexpected!)</td>
</tr>
<tr>
<td><strong>Beauty</strong></td>
<td>-2.6% to -8.5%</td>
<td>-2.1% to -7.6%</td>
<td><strong>Balanced</strong></td>
</tr>
<tr>
<td><strong>Electronics</strong></td>
<td>+0.8% to -6.7%</td>
<td>-2.0% to -6.7%</td>
<td><strong>Balanced/Text</strong></td>
</tr>
</tbody>
</table>
<p><strong>Verdict:</strong> <em><strong>Hypothesis Partially Refuted</strong></em></p>
<ol>
<li>
<p><strong>Clothing (Aesthetic-Centric):</strong> Contrary to our hypothesis, <strong>text dominates</strong> for Clothing recommendations (26-35% drop when removed). While visual features are also important for LATTICE (32.9% drop), MICRO shows virtually no visual dependence (-0.16%). This suggests product descriptions (brand, material, style) carry more predictive signal than images for fashion.</p>
</li>
<li>
<p><strong>Beauty (Aesthetic-Centric):</strong> Both modalities contribute roughly equally (2-8% drops). The hypothesis of visual dominance is not strongly supported — cosmetic product descriptions may capture efficacy claims equally important to visual appearance.</p>
</li>
<li>
<p><strong>Electronics (Functional):</strong> Results are consistent with hypothesis. Text features provide modest benefit (2-7% drops), while visual features can be noise for some models (DiffMM improves when visual removed).</p>
</li>
</ol>
<p><strong>Implications:</strong> For aesthetic domains, textual content (descriptions, attributes) may be more critical than visual features. Domain-specific ablation studies are essential before production deployment.</p>
<hr>
<h4 id="rq2-cold-start-mechanics">RQ2: Cold-Start Mechanics </h4>
<p><strong>Hypothesis:</strong> While DiffMM mitigates user-interaction sparsity via diffusion-based augmentation, we hypothesize that LATTICE and MICRO will demonstrate superior robustness for cold-start items by explicitly leveraging item-item semantic graphs, which are independent of user interaction history.</p>
<p><strong>Validation Method:</strong> Zero-Shot Evaluation on Track 3 (Cold-Start Item) vs. Track 1 (Warm-Start)</p>
<p><strong>Empirical Findings:</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Beauty Cold/Warm</th>
<th>Clothing Cold/Warm</th>
<th>Electronics Cold/Warm</th>
<th>Mean Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DiffMM</strong></td>
<td><strong>120.8%</strong></td>
<td><strong>159.6%</strong></td>
<td><strong>67.3%</strong></td>
<td><strong>115.9%</strong></td>
</tr>
<tr>
<td>LATTICE</td>
<td>18.3%</td>
<td>20.1%</td>
<td>16.7%</td>
<td>18.4%</td>
</tr>
<tr>
<td>MICRO</td>
<td>16.7%</td>
<td>19.8%</td>
<td>11.7%</td>
<td>16.1%</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!CAUTION]<br>
<strong>Critical Result:</strong> Our hypothesis is <strong>completely refuted</strong>. The experimental results are the exact opposite of our prediction.</p>
</blockquote>
<p><strong>Verdict:</strong> <em><strong>Hypothesis Strongly Refuted</strong></em></p>
<ol>
<li>
<p><strong>DiffMM Excels at Cold-Start:</strong> Contrary to our hypothesis, DiffMM achieves Cold/Warm ratios <strong>exceeding 100%</strong> on aesthetic domains, meaning it performs <em>better</em> on cold items than warm items:</p>
<ul>
<li>Clothing: <strong>159.6%</strong> — DiffMM's cold-start performance is 60% better than its warm performance!</li>
<li>Beauty: <strong>120.8%</strong> — consistent 21% improvement on unseen items.</li>
<li>Electronics: <strong>67.3%</strong> — more challenging, but still the only viable cold-start model.</li>
</ul>
<p><strong>Interpretation:</strong> Although all models use the same projection-based inference at test time, DiffMM's <strong>training dynamics</strong> create superior modal representations: (1) the diffusion denoising objective forces projections to capture semantically meaningful content, (2) cross-modal contrastive loss aligns visual/text embeddings, and (3) LeakyReLU activations enable richer non-linear mappings.</p>
</li>
<li>
<p><strong>LATTICE/MICRO Fail Catastrophically:</strong> Both deterministic graph-based methods achieve only <strong>11-20% Cold/Warm ratios</strong> — a <strong>degradation gap of 80-88%</strong>:</p>
<ul>
<li>Beauty MICRO: 16.7% (worst overall)</li>
<li>Electronics MICRO: 11.7% (catastrophic failure)</li>
</ul>
<p><strong>Interpretation:</strong> LATTICE/MICRO use simple linear projections with no non-linear activations for cold-start. Their training objectives do not explicitly optimize for modal generalization—the BPR loss focuses on ID-based collaborative filtering while contrastive losses operate within training items only. The item-item graphs, constructed only from training items, cannot help with truly unseen items.</p>
</li>
<li>
<p><strong>Mechanistic Divergence:</strong> The key difference lies in <strong>training</strong>, not inference:</p>
<ul>
<li>LATTICE/MICRO: Linear projections trained with BPR ± contrastive loss on training items only.</li>
<li>DiffMM: Non-linear projections (LeakyReLU) trained with diffusion reconstruction + cross-modal contrastive loss, creating more transferable representations.</li>
</ul>
</li>
</ol>
<p><strong>Implications:</strong> For cold-start item scenarios, <strong>DiffMM is strongly preferred</strong>. LATTICE and MICRO should only be deployed when item catalog coverage is guaranteed. This finding has significant practical implications for e-commerce systems with frequent new product additions.</p>
<hr>
<h4 id="rq3-architectural-trade-offs">RQ3: Architectural Trade-offs </h4>
<p><strong>Hypothesis:</strong> We hypothesize that DiffMM achieves state-of-the-art accuracy in warm-start scenarios by recovering the user-item interaction manifold, whereas MICRO offers the most stable convergence and robust representations through its contrastive modality alignment.</p>
<p><strong>Validation Method:</strong> Global Benchmarking (Recall@20, NDCG@20) with Convergence Analysis</p>
<p><strong>Empirical Findings (Track 1 - Warm):</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Beauty R@20</th>
<th>Clothing R@20</th>
<th>Electronics R@20</th>
<th>Mean R@20</th>
</tr>
</thead>
<tbody>
<tr>
<td>LATTICE</td>
<td>0.0725</td>
<td><strong>0.0468</strong></td>
<td>0.0855</td>
<td>0.0683</td>
</tr>
<tr>
<td><strong>MICRO</strong></td>
<td><strong>0.0733</strong></td>
<td>0.0461</td>
<td><strong>0.0864</strong></td>
<td><strong>0.0686</strong></td>
</tr>
<tr>
<td>DiffMM</td>
<td>0.0685</td>
<td>0.0413</td>
<td>0.0819</td>
<td>0.0639</td>
</tr>
</tbody>
</table>
<p><strong>Convergence Analysis (from Training Dynamics):</strong></p>
<ul>
<li><strong>MICRO:</strong> Smoothest validation curves with minimal oscillation. Strong performance across all domains.</li>
<li><strong>LATTICE:</strong> Competitive performance, especially on Clothing. Exhibits some oscillation due to k-NN graph updates.</li>
<li><strong>DiffMM:</strong> Underperforms on warm items but shows unique strengths on cold-start (see RQ2).</li>
</ul>
<p><strong>Verdict:</strong> <em><strong>Hypothesis Partially Refuted</strong></em></p>
<ol>
<li>
<p><strong>MICRO Achieves Best Warm Performance:</strong> Contrary to the DiffMM hypothesis, MICRO achieves highest warm recall on Beauty (0.0733) and Electronics (0.0864). DiffMM underperforms on all warm datasets, ranking third overall.</p>
</li>
<li>
<p><strong>MICRO Stability Confirmed:</strong> MICRO exhibits the most stable training dynamics across all datasets, with the smoothest validation curves and lowest variance. Its contrastive objective provides robust gradient signals throughout training.</p>
</li>
<li>
<p><strong>LATTICE Shows Competitive Performance:</strong> LATTICE achieves second-best overall and leads on Clothing (0.0468). The k-NN structure provides beneficial item-item topology.</p>
</li>
<li>
<p><strong>DiffMM Trade-off Revealed:</strong> DiffMM sacrifices warm performance for exceptional cold-start capability. This positions it as a <strong>cold-start specialist</strong> rather than a general-purpose recommender.</p>
</li>
</ol>
<p><strong>Implications:</strong> MICRO remains the recommended default for warm-item scenarios. DiffMM should be deployed specifically for cold-start item scenarios where its generative approach excels.</p>
<hr>
<h4 id="rq4-alignment-correlation">RQ4: Alignment Correlation </h4>
<p><strong>Hypothesis:</strong> We hypothesize that datasets with high Canonical Correlation (CCA) between modalities favor MICRO's contrastive objective, while datasets with weak alignment benefit from LATTICE's disjoint structure learning.</p>
<p><strong>Validation Method:</strong> Correlation Analysis between EDA Modal Alignment Scores and Model Performance (NDCG@20)</p>
<p><strong>Empirical Findings:</strong></p>
<p>From EDA (<code>docs/01_eda.md</code>):</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>CCA Top-3 Mean</th>
<th>Direct Alignment (r)</th>
<th>Best Warm Model</th>
<th>Best Cold Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Beauty</td>
<td>~0.75 (highest)</td>
<td>-0.0009 / 0.025</td>
<td>MICRO</td>
<td><strong>DiffMM</strong></td>
</tr>
<tr>
<td>Clothing</td>
<td>~0.72 (medium)</td>
<td>0.019 / -0.006</td>
<td>LATTICE</td>
<td><strong>DiffMM</strong></td>
</tr>
<tr>
<td>Electronics</td>
<td>~0.68 (lowest)</td>
<td>0.016 / 0.018</td>
<td>MICRO</td>
<td><strong>DiffMM</strong></td>
</tr>
</tbody>
</table>
<p><strong>Observed Correlation:</strong></p>
<ul>
<li>Warm performance: MICRO/LATTICE consistently outperform DiffMM regardless of CCA.</li>
<li>Cold performance: DiffMM dominates across ALL datasets regardless of CCA.</li>
</ul>
<p><strong>Verdict:</strong> <em><strong>Hypothesis Partially Supported, with Cold-Start Revision</strong></em></p>
<ol>
<li>
<p><strong>MICRO-CCA Correlation Confirmed (Warm):</strong> MICRO achieves best warm performance on high-CCA datasets (Beauty, Electronics). The contrastive objective effectively leverages pre-aligned semantic spaces.</p>
</li>
<li>
<p><strong>Cold-Start is Architecture-Dependent, Not CCA-Dependent:</strong> Contrary to our hypothesis, CCA does not predict cold-start performance. <strong>DiffMM dominates cold-start across all CCA levels</strong>, while LATTICE/MICRO fail regardless of modality alignment.</p>
</li>
<li>
<p><strong>Mechanistic Insight:</strong> The cold-start advantage of DiffMM stems from its stochastic sampling (regularization), not modality alignment. Conversely, LATTICE/MICRO's deterministic projections overfit to training distributions.</p>
</li>
</ol>
<p><strong>Implications:</strong> CCA analysis can guide warm model selection (high CCA → MICRO). For cold-start, <strong>always prefer DiffMM</strong> regardless of dataset properties.</p>
<hr>
<h3 id="62-addressing-secondary-research-questions">6.2 Addressing Secondary Research Questions </h3>
<h4 id="rq5-user-sparsity-impact">RQ5: User Sparsity Impact </h4>
<p><strong>Findings:</strong> User sparsity impact varies by model and domain:</p>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Model</th>
<th>Active/Sparse Ratio (R@20)</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Beauty</td>
<td>DiffMM</td>
<td><strong>0.99×</strong></td>
<td>Perfect user fairness</td>
</tr>
<tr>
<td>Beauty</td>
<td>LATTICE/MICRO</td>
<td>1.07-1.34×</td>
<td>Moderate gap</td>
</tr>
<tr>
<td>Clothing</td>
<td>DiffMM</td>
<td><strong>0.79×</strong></td>
<td>Sparse users outperform!</td>
</tr>
<tr>
<td>Clothing</td>
<td>LATTICE/MICRO</td>
<td>1.36×</td>
<td>Active users favored</td>
</tr>
<tr>
<td>Electronics</td>
<td>All models</td>
<td>1.09-1.24×</td>
<td>Moderate gap</td>
</tr>
</tbody>
</table>
<p><strong>Key Insight:</strong> DiffMM achieves remarkable <strong>user fairness</strong> across all domains, with ratios near or below 1.0×. On Clothing, DiffMM actually performs <em>better</em> on sparse users than active users (0.79×). This confirms that DiffMM's diffusion-based augmentation effectively compensates for sparse user histories, validating its design for user-interaction sparsity mitigation. LATTICE and MICRO consistently favor active users (1.07-1.36× ratios).</p>
<hr>
<h3 id="63-summary-of-key-contributions">6.3 Summary of Key Contributions </h3>
<table>
<thead>
<tr>
<th>Finding</th>
<th>Evidence</th>
<th>Implication</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DiffMM excels at cold-start</strong></td>
<td>120-160% Cold/Warm ratios</td>
<td>Deploy DiffMM for new product recommendations</td>
</tr>
<tr>
<td><strong>LATTICE/MICRO fail at cold-start</strong></td>
<td>11-20% Cold/Warm ratios</td>
<td>Avoid for catalogs with frequent new items</td>
</tr>
<tr>
<td><strong>MICRO is best for warm items</strong></td>
<td>Highest R@20 on 2/3 warm datasets</td>
<td>Deploy MICRO when item coverage is guaranteed</td>
</tr>
<tr>
<td><strong>Clothing is text-dominant</strong></td>
<td>26-35% drop when text removed</td>
<td>Prioritize product descriptions for fashion</td>
</tr>
<tr>
<td><strong>DiffMM is most user-fair</strong></td>
<td>0.79-0.99× Active/Sparse ratios</td>
<td>Deploy DiffMM for sparse user populations</td>
</tr>
<tr>
<td><strong>Architecture &gt; Modality Alignment</strong></td>
<td>DiffMM cold-start dominates regardless of CCA</td>
<td>Stochastic sampling is key to generalization</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="references">References </h2>
<ol>
<li><strong>Zhang et al. (2021)</strong> - LATTICE: Mining Latent Structures for Multimodal Recommendation</li>
<li><strong>Zhang et al. (2022)</strong> - MICRO: Contrastive Multimodal Recommendation</li>
<li><strong>Jiang et al. (2023)</strong> - DiffMM: Diffusion Model for Multimodal Recommendation</li>
<li><strong>Xu et al. (2025)</strong> - Multimodal Recommender Systems: A Survey</li>
</ol>
<hr>
<p><em>Documentation generated for the Multimodal Recommendation System Evaluation Framework</em></p>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>