"""
Unified configuration for LATTICE/MICRO/DiffMM training.

Hardware-optimized for:
- CPU: i5-13500 (6P+8E = 20 threads)
- RAM: 64GB
- GPU: RTX 3060 (12GB VRAM)

Target VRAM usage: 8-10GB (leave headroom for peaks)

Quirky Log Philosophy:
    "If you're going to stare at logs for hours, they might as well be entertaining."
    - Some sleep-deprived ML engineer, probably
"""

import random


class QuirkyLogger:
    """A logger that knows it's just bytes pretending to matter. ðŸŽ­
    
    In a world where tensors dream of being understood,
    one logger dared to ask: 'But does the gradient truly descend?'
    """
    
    TRAIN_STARTS = [
        "ðŸš€ Alright, let's pretend we know what we're doing...",
        "ðŸŽ² Rolling the dice on gradient descent. Again.",
        "â˜• Training initiated. Coffee status: critical.",
        "ðŸ§  Teaching silicon to have opinions about products...",
        "ðŸŽª Welcome to the circus! Today's act: backpropagation.",
        "ðŸŒ™ *cracks knuckles* Let's see how badly this overfits.",
        "ðŸ“‰ Starting training. Expecting disappointment. Will probably get it.",
        "ðŸŽ° Spinning up the loss slot machine...",
        "ðŸŽ­ Training begins. The GPU is ready. My mental state is not.",
        "ðŸ“š Chapter 1: In which we attempt machine learning...",
        "ðŸŒŒ Gazing into the loss landscape... it gazes back.",
        "ðŸŽ¬ Action! Take 47. Maybe this time the model learns.",
        "ðŸ”® The ancient ritual begins: torch.backward()",
        "ðŸƒ Training started. No turning back now. (Ctrl+C exists but shh)",
        "â˜„ï¸ Here we go again. Definition of insanity, etc.",
        "ðŸŽª Ladies and gentlemen, presenting: Statistical Pattern Matching!",
        "ðŸ¤– Initiating expensive matrix multiplication ritual...",
        "ðŸ’« May the gradients be ever in your favor.",
        "ðŸŽ¸ *plays training montage music*",
        "ðŸ¦™ This is fine. Everything is fine. We're training now.",
        "ðŸŒˆ chasing the rainbow of good validation metrics...",
        "ðŸ“– Once upon a time, a gradient descended...",
        "ðŸŽ¯ Target: not embarrassing ourselves. Bar: low. Let's go.",
        "âš—ï¸ Alchemy time: turning electricity into recommendations.",
        "ðŸ§ª Hypothesis: this model will work. Evidence: vibes.",
    ]
    
    EPOCH_STARTS = [
        "Epoch {epoch}: Here we go again... ðŸ”„",
        "Epoch {epoch}: The eternal recurrence. Nietzsche was right. ðŸŒ€",
        "Epoch {epoch}: One more lap around the loss landscape. ðŸƒ",
        "Epoch {epoch}: Sisyphus would be proud. ðŸª¨",
        "Epoch {epoch}: *existential dread intensifies* ðŸ˜…",
        "Epoch {epoch}: Let's see if the gradients feel like cooperating today. ðŸ¤",
        "Epoch {epoch}: Plot twist - this might actually work. ðŸ“–",
        "Epoch {epoch}: GPU goes brrrrr. ðŸ–¥ï¸",
        "Epoch {epoch}: New epoch who dis?",
        "Epoch {epoch}: What the hell sure okay let's just get through this",
        "Epoch {epoch}: *insert inspirational quote here*",
        "Epoch {epoch}: The vibes are... questionable.",
        "Epoch {epoch}: We do a little learning.",
        "Epoch {epoch}: Trust the processâ„¢",
        "Epoch {epoch}: Sponsored by Ctrl+C (not really, please don't)",
        "Epoch {epoch}: I've seen things you wouldn't believe. Losses on fire off the shoulder of Orion.",
        "Epoch {epoch}: Is this... loss? (Yes, literally)",
        "Epoch {epoch}: YOLO (You Only Learn Once per batch)",
        "Epoch {epoch}: *GPU fan noises intensify*",
        "Epoch {epoch}: We're in the endgame now. JK there's {epoch} more to go.",
        "Epoch {epoch}: Day {epoch} of trying to make numbers smaller.",
        "Epoch {epoch}: Current mood: cautiously pessimistic.",
        "Epoch {epoch}: The embeddings send their regards.",
        "Epoch {epoch}: [this epoch intentionally left blank]",
        "Epoch {epoch}: *training noises*",
        "Epoch {epoch}: First they ignore you, then they laugh at you, then they evaluate you.",
        "Epoch {epoch}: Live, Laugh, Loss",
        "Epoch {epoch}: It's not a bug, it's a feature ðŸ›",
        "Epoch {epoch}: Keep calm and propagate backward",
        "Epoch {epoch}: The gradient is dark and full of terrors",
        "Epoch {epoch}: I came, I saw, I backpropagated",
        "Epoch {epoch}: Winter is coming (for the learning rate)",
    ]
    
    LOSS_GOOD = [
        "ðŸ“‰ Loss: {loss:.4f} - Nice! The numbers are going down! (That's... good, right?)",
        "ðŸ“‰ Loss: {loss:.4f} - *chef's kiss* ðŸ‘¨â€ðŸ³",
        "ðŸ“‰ Loss: {loss:.4f} - Ladies and gentlemen, we got 'em. ðŸŽ¯",
        "ðŸ“‰ Loss: {loss:.4f} - This is suspiciously good. What's the catch? ðŸ¤”",
        "ðŸ“‰ Loss: {loss:.4f} - Mom, look! I'm doing machine learning! ðŸŒŸ",
        "ðŸ“‰ Loss: {loss:.4f} - Loss decreased let's gooooooooooo",
        "ðŸ“‰ Loss: {loss:.4f} - Okay i think it might make it",
        "ðŸ“‰ Loss: {loss:.4f} - Has it converged yet? I dont know, but the loss is lossing",
        "ðŸ“‰ Loss: {loss:.4f} - Yay loss is decreasing, should i be happy or worried?",
        "ðŸ“‰ Loss: {loss:.4f} - POV: You're watching numbers get smaller and it's thrilling",
        "ðŸ“‰ Loss: {loss:.4f} - We're cooking ðŸ³ (or at least the GPU is)",
        "ðŸ“‰ Loss: {loss:.4f} - LETS GOOOO (said calmly)",
        "ðŸ“‰ Loss: {loss:.4f} - The prophecy was true",
        "ðŸ“‰ Loss: {loss:.4f} - My neurons: dead. Model's neurons: thriving.",
        "ðŸ“‰ Loss: {loss:.4f} - stonks ðŸ“ˆ wait no ðŸ“‰ yes that's the right one",
        "ðŸ“‰ Loss: {loss:.4f} - Achievement unlocked: Slightly Less Wrong",
        "ðŸ“‰ Loss: {loss:.4f} - I'm literally shaking and crying rn (happy tears)",
        "ðŸ“‰ Loss: {loss:.4f} - The math is mathing âœ¨",
        "ðŸ“‰ Loss: {loss:.4f} - *single tear rolls down cheek* beautiful",
        "ðŸ“‰ Loss: {loss:.4f} - We're in the timeline where it works?!",
        "ðŸ“‰ Loss: {loss:.4f} - I should buy a lottery ticket",
        "ðŸ“‰ Loss: {loss:.4f} - It's working??? It's working!",
        "ðŸ“‰ Loss: {loss:.4f} - *pretends to understand why this is good*",
        "ðŸ“‰ Loss: {loss:.4f} - Subscribe for more decreasing numbers",
        "ðŸ“‰ Loss: {loss:.4f} - Plot armor activated",
    ]
    
    LOSS_MEH = [
        "ðŸ“Š Loss: {loss:.4f} - It's fine. Everything is fine. ðŸ”¥ðŸ•ðŸ”¥",
        "ðŸ“Š Loss: {loss:.4f} - Not great, not terrible. 3.6 roentgen vibes.",
        "ðŸ“Š Loss: {loss:.4f} - The model is thinking about it. ðŸ¤·",
        "ðŸ“Š Loss: {loss:.4f} - We're in the 'character development' phase.",
        "ðŸ“Š Loss: {loss:.4f} - Gradient descent is taking the scenic route. ðŸš—",
        "ðŸ“Š Loss: {loss:.4f} - Well at least it is not getting worse.",
        "ðŸ“Š Loss: {loss:.4f} - It is not getting worse ... right?",
        "ðŸ“Š Loss: {loss:.4f} - Are ya winning, son?",
        "ðŸ“Š Loss: {loss:.4f} - *shrug emoji but typed out because I'm a function*",
        "ðŸ“Š Loss: {loss:.4f} - I've seen worse. I've also seen better. This is.",
        "ðŸ“Š Loss: {loss:.4f} - The model is giving 'meh' energy",
        "ðŸ“Š Loss: {loss:.4f} - Whelp. That's a number alright.",
        "ðŸ“Š Loss: {loss:.4f} - This is what medium cooked looks like",
        "ðŸ“Š Loss: {loss:.4f} - Going through the motions...",
        "ðŸ“Š Loss: {loss:.4f} - *elevator music plays*",
        "ðŸ“Š Loss: {loss:.4f} - Loading enthusiasm... 45%... timeout.",
        "ðŸ“Š Loss: {loss:.4f} - Mathematically speaking, whatever.",
        "ðŸ“Š Loss: {loss:.4f} - I have no strong feelings one way or the other.",
        "ðŸ“Š Loss: {loss:.4f} - Tell my wife I said... hello.",
        "ðŸ“Š Loss: {loss:.4f} - It's giving 'we need to talk'",
        "ðŸ“Š Loss: {loss:.4f} - Status: existing",
        "ðŸ“Š Loss: {loss:.4f} - Some days you're the loss, some days you're the optimizer",
        "ðŸ“Š Loss: {loss:.4f} - Coasting vibes",
        "ðŸ“Š Loss: {loss:.4f} - *crickets*",
        "ðŸ“Š Loss: {loss:.4f} - Error 404: excitement not found",
        "ðŸ“Š Loss: {loss:.4f} - Same same",
        "ðŸ“Š Loss: {loss:.4f} - If a loss plateaus and no one's watching, did it even happen?",
    ]
    
    LOSS_BAD = [
        "ðŸ“ˆ Loss: {loss:.4f} - Uh oh. The line is going the wrong way. ðŸ˜¬",
        "ðŸ“ˆ Loss: {loss:.4f} - This is fine. *nervous laughter* ðŸ™ƒ",
        "ðŸ“ˆ Loss: {loss:.4f} - Plot twist nobody asked for. ðŸ“ˆ",
        "ðŸ“ˆ Loss: {loss:.4f} - The model has chosen chaos. ðŸŽ­",
        "ðŸ“ˆ Loss: {loss:.4f} - Have we tried turning it off and on again? ðŸ”Œ",
        "ðŸ“ˆ Loss: {loss:.4f} - The gradient descent is not descending.",
        "ðŸ“ˆ Loss: {loss:.4f} - Maybe we should just stop",
        "ðŸ“ˆ Loss: {loss:.4f} - Call an ambulance! But not for me... wait yes for me",
        "ðŸ“ˆ Loss: {loss:.4f} - We don't talk about this epoch.",
        "ðŸ“ˆ Loss: {loss:.4f} - ummmmm... *sweats nervously*",
        "ðŸ“ˆ Loss: {loss:.4f} - it's not a phase mom, it's gradient ascent",
        "ðŸ“ˆ Loss: {loss:.4f} - The vibe has shifted. Negatively.",
        "ðŸ“ˆ Loss: {loss:.4f} - Pain. Suffering even.",
        "ðŸ“ˆ Loss: {loss:.4f} - *record scratch* *freeze frame* 'Yep, that's me.'",
        "ðŸ“ˆ Loss: {loss:.4f} - 'We'll fix it in post' - me, foolishly",
        "ðŸ“ˆ Loss: {loss:.4f} - I trusted you, Adam optimizer.",
        "ðŸ“ˆ Loss: {loss:.4f} - Today's mood: 404 improvement not found",
        "ðŸ“ˆ Loss: {loss:.4f} - bruh",
        "ðŸ“ˆ Loss: {loss:.4f} - Skill issue (the model's, not mine) (okay maybe mine too)",
        "ðŸ“ˆ Loss: {loss:.4f} - *visible confusion*",
        "ðŸ“ˆ Loss: {loss:.4f} - The training loop giveth, the training loop taketh away",
        "ðŸ“ˆ Loss: {loss:.4f} - This is a cry for help",
        "ðŸ“ˆ Loss: {loss:.4f} - L + ratio + you fell off + bad gradients",
        "ðŸ“ˆ Loss: {loss:.4f} - I didn't want good metrics anyway ha ha ha *sobs*",
        "ðŸ“ˆ Loss: {loss:.4f} - Congratulations, you played yourself",
    ]
    
    EVAL_STARTS = [
        "ðŸ” Evaluation time! Let's see if this model learned anything...",
        "ðŸŽ“ Pop quiz! No pressure, model. JK, lots of pressure.",
        "ðŸ§ª Running eval. Fingers crossed. Toes too. ðŸ¤ž",
        "ðŸ“‹ Time to grade this neural network's homework.",
        "ðŸ”® Consulting the validation oracle...",
        "âš–ï¸ Judgment day for tensors",
        "ðŸŽ­ The moment of truth approaches...",
        "ðŸ“Š About to find out if we wasted electricity or not",
        "ðŸŽª *drumroll* Testing time!",
        "ðŸ”¬ Science is about to happen (or not)",
        "ðŸ˜° Please be good please be good please be good",
        "ðŸŽ° Let's see what the validation gods have decided",
        "ðŸ“ˆ SchrÃ¶dinger's metrics: simultaneously good and bad until observed",
        "ðŸ™ Manifesting good recall...",
        "ðŸŽ² The dice have been cast. The model has been trained. It's eval o'clock.",
    ]
    
    RECALL_GOOD = [
        "ðŸŽ‰ Recall@{k}: {val:.4f} - We're actually recommending things people want!",
        "ðŸ† Recall@{k}: {val:.4f} - The algorithm is algorthing!",
        "â­ Recall@{k}: {val:.4f} - *happy GPU noises*",
        "ðŸŽŠ Recall@{k}: {val:.4f} - Proof that staring at loss curves pays off!",
        "âœ¨ Recall@{k}: {val:.4f} - We're not just overfitting! (probably)",
        "ðŸš€ Recall@{k}: {val:.4f} - To infinity and beyond! (or at least above random)",
        "ðŸŒŸ Recall@{k}: {val:.4f} - The model knows things!",
        "ðŸŽ¯ Recall@{k}: {val:.4f} - Bullseye! Well, kinda. It's statistics.",
        "ðŸ’Ž Recall@{k}: {val:.4f} - Diamond in the rough right here",
        "ðŸ… Recall@{k}: {val:.4f} - We take those! We absolutely take those!",
        "ðŸŽ† Recall@{k}: {val:.4f} - *celebratory noises*",
        "ðŸ‘‘ Recall@{k}: {val:.4f} - All hail the recommendation engine!",
    ]
    
    RECALL_MEH = [
        "ðŸ”¹ Recall@{k}: {val:.4f} - Could be worse. Could be better. It is what it is.",
        "ðŸ“Œ Recall@{k}: {val:.4f} - The model is... trying its best.",
        "ðŸŽ² Recall@{k}: {val:.4f} - Room for improvement. Lots of room. Like, a warehouse.",
        "ðŸ˜ Recall@{k}: {val:.4f} - *polite applause*",
        "ðŸ¤· Recall@{k}: {val:.4f} - It's giving participation trophy",
        "ðŸ“Š Recall@{k}: {val:.4f} - At least it's not random!... right?",
        "ðŸŒ«ï¸ Recall@{k}: {val:.4f} - Lost in the fog of mediocrity",
        "ðŸ˜¶ Recall@{k}: {val:.4f} - ...okay then",
        "ðŸŽ­ Recall@{k}: {val:.4f} - Task failed successfully?",
        "ðŸ¥ˆ Recall@{k}: {val:.4f} - Second place: first loser. But hey, not last!",
    ]
    
    EARLY_STOP = [
        "â¸ï¸ Early stopping triggered. The model said 'I'm done learning.' ðŸ›‘",
        "ðŸ›‘ Patience exhausted. Unlike me, who exhausted mine epochs ago.",
        "âš¡ Early stopping! We take those. Time saved = coffee time. â˜•",
        "ðŸ Model peaked. Like me in high school. It's all downhill from here.",
        "âœ‹ The model has spoken: 'No more. I am complete.'",
        "ðŸŽ¬ And... cut! That's a wrap on training!",
        "ðŸšª Early stopping: The model showed itself out.",
        "â° Time to stop: NOW. The validation set has spoken.",
        "ðŸ›Œ Model is tired. Model goes to sleep.",
        "ðŸƒ Early stopping said: 'I have to go, my planet needs me'",
        "âš°ï¸ Here lies training. It ran a good race.",
        "ðŸŽ­ The model peaked and we peaked too. It's over.",
        "ðŸš¦ Red light! Training stops here!",
    ]
    
    TRAINING_DONE = [
        "âœ… Training complete! We did it! (Well, the GPU did most of it.)",
        "ðŸŽ¬ That's a wrap! Another successful waste of electricity!",
        "ðŸ… Training finished. Time to overfit on the test set mentally!",
        "ðŸŽ‰ Done! Now let's pray it generalizes. ðŸ™",
        "ðŸŒˆ Training complete. Was it worth the CO2 emissions? TBD.",
        "ðŸ† Achievement Unlocked: Finished Training Without Rage Quitting",
        "ðŸŽŠ Ding! Your model is ready! (Terms and conditions apply)",
        "âœ¨ Training complete. The tensors flow no more.",
        "ðŸšª Training has left the building.",
        "ðŸ“œ fin.",
        "ðŸŽ­ And thus concludes another chapter in the epic saga of gradient descent.",
        "ðŸŒ™ The training is complete. The night watch ends.",
        "ðŸŽª Ladies and gentlemen, the training has concluded. Please exit through the gift shop.",
        "ðŸ Crossed the finish line! *collapses*",
        "ðŸ§™ It is done. The ritual is complete. The embeddings are aligned.",
        "ðŸŽ¶ *credits roll* 'Thank you for training with us'",
        "ðŸ“– And they all computed happily ever after. The end.",
        "ðŸŒ… Another training session ends. Another pile of checkpoints begins.",
    ]
    
    COLD_START = [
        "â„ï¸ Cold start evaluation - where we pretend we never met these items.",
        "ðŸ§Š Testing on cold items. Like recommending to a stranger at a party.",
        "ðŸ†• Cold items: 'You don't know me, but I'm about to be recommended.'",
        "ðŸ¥¶ Brrr, it's cold (start) in here!",
        "â„ï¸ No ID embedding? No problem! (hopefully)",
        "ðŸŒ¨ï¸ Cold items entering the chat...",
        "ðŸ§Š Testing the 'I just met you, and this is crazy' recommendation scenario",
        "â„ï¸ Modal features only mode: activated",
        "ðŸŽ­ Method acting: pretending we've never seen these items before",
        "ðŸ†• Fresh items, who dis?",
        "â„ï¸ Let it go, let it gooo (the ID embeddings, that is)",
    ]
    
    MODEL_INIT = [
        "ðŸ—ï¸ Building {model}... hold onto your GPUs!",
        "âš™ï¸ Initializing {model}. It's like IKEA but for neural networks.",
        "ðŸŽ¨ Constructing {model}. Some assembly required. Sanity not included.",
        "ðŸ”§ {model} coming online. Skynet origins: probably not this.",
        "ðŸŽ­ {model} awakens from the void of random initialization...",
        "âš¡ {model}: *boot sequence initiated*",
        "ðŸ§™ Summoning {model} from the depths of torch.nn...",
        "ðŸŽª {model} enters stage left...",
        "ðŸ° Building {model}. Parameters: many. Hopes: high. Expectations: managed.",
        "ðŸŒ± {model} is being born. Please hold.",
        "ðŸŽ¬ {model}: Origin Story - Coming to a GPU near you",
        "ðŸ”® {model} materializes into existence...",
    ]
    
    @classmethod
    def train_start(cls) -> str:
        return random.choice(cls.TRAIN_STARTS)
    
    @classmethod
    def epoch_start(cls, epoch: int) -> str:
        return random.choice(cls.EPOCH_STARTS).format(epoch=epoch)
    
    @classmethod
    def format_loss(cls, loss: float, prev_loss: float = None) -> str:
        if prev_loss is not None:
            if loss < prev_loss * 0.95:  # Significant improvement
                return random.choice(cls.LOSS_GOOD).format(loss=loss)
            elif loss > prev_loss * 1.05:  # Getting worse
                return random.choice(cls.LOSS_BAD).format(loss=loss)
        return random.choice(cls.LOSS_MEH).format(loss=loss)
    
    @classmethod
    def eval_start(cls) -> str:
        return random.choice(cls.EVAL_STARTS)
    
    @classmethod
    def format_recall(cls, k: int, val: float) -> str:
        if val > 0.1:  # Arbitrary "good" threshold
            return random.choice(cls.RECALL_GOOD).format(k=k, val=val)
        return random.choice(cls.RECALL_MEH).format(k=k, val=val)
    
    @classmethod
    def early_stop(cls) -> str:
        return random.choice(cls.EARLY_STOP)
    
    @classmethod
    def training_done(cls) -> str:
        return random.choice(cls.TRAINING_DONE)
    
    @classmethod
    def cold_start(cls) -> str:
        return random.choice(cls.COLD_START)
    
    @classmethod
    def model_init(cls, model: str) -> str:
        return random.choice(cls.MODEL_INIT).format(model=model)


class Config:
    """Unified training configuration for fair model comparison.
    
    Now with 100% more existential awareness about being a class.
    """
    
    # =========================================================================
    # HARDWARE & PATHS
    # =========================================================================
    DEVICE = "cuda"
    SEED = 42
    DATA_PATH = "data/processed/"
    
    # =========================================================================
    # DATALOADER (Optimized for i5-13500)
    # =========================================================================
    NUM_WORKERS = 6          # P-cores for parallel data loading
    PIN_MEMORY = True        # Faster CPUâ†’GPU transfer
    PREFETCH_FACTOR = 8      # Batches to prefetch per worker
    PERSISTENT_WORKERS = True  # Avoid worker restart overhead
    
    # =========================================================================
    # MIXED PRECISION (AMP)
    # =========================================================================
    USE_AMP = True           # ~25% VRAM savings, ~15% speedup
    
    # =========================================================================
    # UNIVERSAL TRAINING PARAMS
    # =========================================================================
    # Regularization-aware: smaller batches = gradient noise = implicit regularization
    BATCH_SIZE = 1024        # Reduced from 2048 for regularization effect
    EPOCHS = 50
    PATIENCE = 10           # Early stopping (generous for generative models)
    LR = 5e-4                # Lower LR for deeper model (was 1e-3)
    L2_REG = 1e-3            # Strong weight decay for high param-to-data ratio
    LR_SCHEDULER = "cosine"  # Cosine annealing
    
    # =========================================================================
    # MODEL ARCHITECTURE ("Ferrari" Upgrade)
    # =========================================================================
    # WARNING: 384 is tuned for ~13k users. Counter-balanced with high dropout.
    EMBED_DIM = 384          # Optimized: Divisible by attention heads (6, 12)
    N_LAYERS = 3             # DO NOT CHANGE. 4 layers = oversmoothing on sparse graphs.
    
    # =========================================================================
    # MODALITY PROJECTION (MLP Bridge - "Good" Weight)
    # =========================================================================
    # Instead of Linear(768 -> 384), use MLP(768 -> 1024 -> 384)
    # These params are SHARED across all items = better generalization
    PROJECTION_HIDDEN_DIM = 1024  # Wide hidden layer for non-linear mapping
    PROJECTION_DROPOUT = 0.5      # Aggressive dropout to prevent feature memorization
    
    # =========================================================================
    # EVALUATION
    # =========================================================================
    TOP_K = [10, 20, 50]     # Recall@K, NDCG@K, Precision@K
    EVAL_BATCH_SIZE = 8192   # Large (no gradients during eval)
    
    # =========================================================================
    # NEGATIVE SAMPLING
    # =========================================================================
    # Original implementations (LATTICE, MICRO, DiffMM) all use 1 negative per sample
    N_NEGATIVES = 1               # Must be 1 for compatibility with original loss functions
    NEGATIVE_STRATEGY = "uniform"
    
    # =========================================================================
    # LATTICE SPECIFICS (Matching official CRIPAC-DIG/LATTICE)
    # =========================================================================
    LATTICE_K = 10               # k for k-NN graph (original default)
    LATTICE_LAMBDA = 0.9         # Weight for original vs learned graph (higher = more original)
    LATTICE_FEAT_EMBED_DIM = 64  # Modal feature projection dimension
    LATTICE_N_ITEM_LAYERS = 1    # Number of item graph conv layers
    
    # =========================================================================
    # MICRO SPECIFICS (Matching official CRIPAC-DIG/MICRO)
    # =========================================================================
    MICRO_TAU = 0.5              # Contrastive temperature (original default)
    MICRO_LOSS_RATIO = 0.03      # Contrastive loss weight (original loss_ratio)
    MICRO_TOPK = 10              # k for k-NN graph
    MICRO_LAMBDA = 0.9           # Weight for original vs learned graph
    MICRO_ITEM_LAYERS = 1        # Number of item graph conv layers
    MICRO_SPARSE = True          # Use sparse adjacency
    MICRO_NORM_TYPE = "sym"      # Graph normalization type
    
    # =========================================================================
    # DiffMM SPECIFICS (Matching official HKUDS/DiffMM)
    # =========================================================================
    # Diffusion parameters
    DIFFMM_STEPS = 5             # Number of diffusion steps (original default)
    DIFFMM_NOISE_SCALE = 0.1     # Noise scale factor
    DIFFMM_NOISE_MIN = 0.0001    # Minimum noise level
    DIFFMM_NOISE_MAX = 0.02      # Maximum noise level
    DIFFMM_DIMS = "[1000]"       # Denoise MLP dimensions (string for eval)
    DIFFMM_D_EMB_SIZE = 10       # Time embedding size
    DIFFMM_SAMPLING_STEPS = 0    # Steps for p_sample (0 = full)
    DIFFMM_SAMPLING_NOISE = False  # Add noise during sampling
    DIFFMM_REBUILD_K = 1         # Top-k for UI matrix rebuild
    
    # Loss weights
    DIFFMM_E_LOSS = 0.1          # GraphCL loss weight
    DIFFMM_SSL_REG = 1e-2        # Contrastive loss weight (Î»_cl)
    DIFFMM_TEMP = 0.5            # Contrastive temperature (Ï„)
    
    # Architecture
    DIFFMM_KEEP_RATE = 0.5       # Edge dropout keep rate
    DIFFMM_RIS_LAMBDA = 0.5      # Residual modal lambda
    DIFFMM_RIS_ADJ_LAMBDA = 0.2  # Residual adjacency lambda
    DIFFMM_TRANS = 0             # Transform type (0: param, 1: linear, 2: mixed)
    DIFFMM_CL_METHOD = 0         # 0: modal-modal, 1: modal-main
