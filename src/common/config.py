"""
Unified configuration for LATTICE/MICRO/DiffMM training.

Hardware-optimized for:
- CPU: i5-13500 (6P+8E = 20 threads)
- RAM: 64GB
- GPU: RTX 3060 (12GB VRAM)

Target VRAM usage: 8-10GB (leave headroom for peaks)

Quirky Log Philosophy:
    "If you're going to stare at logs for hours, they might as well be entertaining."
    - Some sleep-deprived ML engineer, probably
"""

import random


class QuirkyLogger:
    """A logger that knows it's just bytes pretending to matter. ðŸŽ­"""
    
    TRAIN_STARTS = [
        "ðŸš€ Alright, let's pretend we know what we're doing...",
        "ðŸŽ² Rolling the dice on gradient descent. Again.",
        "â˜• Training initiated. Coffee status: critical.",
        "ðŸ§  Teaching silicon to have opinions about products...",
        "ðŸŽª Welcome to the circus! Today's act: backpropagation.",
        "ðŸŒ™ *cracks knuckles* Let's see how badly this overfits.",
        "ðŸ“‰ Starting training. Expecting disappointment. Will probably get it.",
        "ðŸŽ° Spinning up the loss slot machine...",
    ]
    
    EPOCH_STARTS = [
        "Epoch {epoch}: Here we go again... ðŸ”„",
        "Epoch {epoch}: The eternal recurrence. Nietzsche was right. ðŸŒ€",
        "Epoch {epoch}: One more lap around the loss landscape. ðŸƒ",
        "Epoch {epoch}: Sisyphus would be proud. ðŸª¨",
        "Epoch {epoch}: *existential dread intensifies* ðŸ˜…",
        "Epoch {epoch}: Let's see if the gradients feel like cooperating today. ðŸ¤",
        "Epoch {epoch}: Plot twist - this might actually work. ðŸ“–",
        "Epoch {epoch}: GPU goes brrrrr. ðŸ–¥ï¸",
    ]
    
    LOSS_GOOD = [
        "ðŸ“‰ Loss: {loss:.4f} - Nice! The numbers are going down! (That's... good, right?)",
        "ðŸ“‰ Loss: {loss:.4f} - *chef's kiss* ðŸ‘¨â€ðŸ³",
        "ðŸ“‰ Loss: {loss:.4f} - Ladies and gentlemen, we got 'em. ðŸŽ¯",
        "ðŸ“‰ Loss: {loss:.4f} - This is suspiciously good. What's the catch? ðŸ¤”",
        "ðŸ“‰ Loss: {loss:.4f} - Mom, look! I'm doing machine learning! ðŸŒŸ",
    ]
    
    LOSS_MEH = [
        "ðŸ“Š Loss: {loss:.4f} - It's fine. Everything is fine. ðŸ”¥ðŸ•ðŸ”¥",
        "ðŸ“Š Loss: {loss:.4f} - Not great, not terrible. 3.6 roentgen vibes.",
        "ðŸ“Š Loss: {loss:.4f} - The model is thinking about it. ðŸ¤·",
        "ðŸ“Š Loss: {loss:.4f} - We're in the 'character development' phase.",
        "ðŸ“Š Loss: {loss:.4f} - Gradient descent is taking the scenic route. ðŸš—",
    ]
    
    LOSS_BAD = [
        "ðŸ“ˆ Loss: {loss:.4f} - Uh oh. The line is going the wrong way. ðŸ˜¬",
        "ðŸ“ˆ Loss: {loss:.4f} - This is fine. *nervous laughter* ðŸ™ƒ",
        "ðŸ“ˆ Loss: {loss:.4f} - Plot twist nobody asked for. ðŸ“ˆ",
        "ðŸ“ˆ Loss: {loss:.4f} - The model has chosen chaos. ðŸŽ­",
        "ðŸ“ˆ Loss: {loss:.4f} - Have we tried turning it off and on again? ðŸ”Œ",
    ]
    
    EVAL_STARTS = [
        "ðŸ” Evaluation time! Let's see if this model learned anything...",
        "ðŸŽ“ Pop quiz! No pressure, model. JK, lots of pressure.",
        "ðŸ§ª Running eval. Fingers crossed. Toes too. ðŸ¤ž",
        "ðŸ“‹ Time to grade this neural network's homework.",
        "ðŸ”® Consulting the validation oracle...",
    ]
    
    RECALL_GOOD = [
        "ðŸŽ‰ Recall@{k}: {val:.4f} - We're actually recommending things people want!",
        "ðŸ† Recall@{k}: {val:.4f} - The algorithm is algorthing!",
        "â­ Recall@{k}: {val:.4f} - *happy GPU noises*",
        "ðŸŽŠ Recall@{k}: {val:.4f} - Proof that staring at loss curves pays off!",
    ]
    
    RECALL_MEH = [
        "ðŸ”¹ Recall@{k}: {val:.4f} - Could be worse. Could be better. It is what it is.",
        "ðŸ“Œ Recall@{k}: {val:.4f} - The model is... trying its best.",
        "ðŸŽ² Recall@{k}: {val:.4f} - Room for improvement. Lots of room. Like, a warehouse.",
    ]
    
    EARLY_STOP = [
        "â¸ï¸ Early stopping triggered. The model said 'I'm done learning.' ðŸ›‘",
        "ðŸ›‘ Patience exhausted. Unlike me, who exhausted mine epochs ago.",
        "âš¡ Early stopping! We take those. Time saved = coffee time. â˜•",
        "ðŸ Model peaked. Like me in high school. It's all downhill from here.",
    ]
    
    TRAINING_DONE = [
        "âœ… Training complete! We did it! (Well, the GPU did most of it.)",
        "ðŸŽ¬ That's a wrap! Another successful waste of electricity!",
        "ðŸ… Training finished. Time to overfit on the test set mentally!",
        "ðŸŽ‰ Done! Now let's pray it generalizes. ðŸ™",
        "ðŸŒˆ Training complete. Was it worth the CO2 emissions? TBD.",
    ]
    
    COLD_START = [
        "â„ï¸ Cold start evaluation - where we pretend we never met these items.",
        "ðŸ§Š Testing on cold items. Like recommending to a stranger at a party.",
        "ðŸ†• Cold items: 'You don't know me, but I'm about to be recommended.'",
    ]
    
    MODEL_INIT = [
        "ðŸ—ï¸ Building {model}... hold onto your GPUs!",
        "âš™ï¸ Initializing {model}. It's like IKEA but for neural networks.",
        "ðŸŽ¨ Constructing {model}. Some assembly required. Sanity not included.",
        "ðŸ”§ {model} coming online. Skynet origins: probably not this.",
    ]
    
    @classmethod
    def train_start(cls) -> str:
        return random.choice(cls.TRAIN_STARTS)
    
    @classmethod
    def epoch_start(cls, epoch: int) -> str:
        return random.choice(cls.EPOCH_STARTS).format(epoch=epoch)
    
    @classmethod
    def format_loss(cls, loss: float, prev_loss: float = None) -> str:
        if prev_loss is not None:
            if loss < prev_loss * 0.95:  # Significant improvement
                return random.choice(cls.LOSS_GOOD).format(loss=loss)
            elif loss > prev_loss * 1.05:  # Getting worse
                return random.choice(cls.LOSS_BAD).format(loss=loss)
        return random.choice(cls.LOSS_MEH).format(loss=loss)
    
    @classmethod
    def eval_start(cls) -> str:
        return random.choice(cls.EVAL_STARTS)
    
    @classmethod
    def format_recall(cls, k: int, val: float) -> str:
        if val > 0.1:  # Arbitrary "good" threshold
            return random.choice(cls.RECALL_GOOD).format(k=k, val=val)
        return random.choice(cls.RECALL_MEH).format(k=k, val=val)
    
    @classmethod
    def early_stop(cls) -> str:
        return random.choice(cls.EARLY_STOP)
    
    @classmethod
    def training_done(cls) -> str:
        return random.choice(cls.TRAINING_DONE)
    
    @classmethod
    def cold_start(cls) -> str:
        return random.choice(cls.COLD_START)
    
    @classmethod
    def model_init(cls, model: str) -> str:
        return random.choice(cls.MODEL_INIT).format(model=model)


class Config:
    """Unified training configuration for fair model comparison.
    
    Now with 100% more existential awareness about being a class.
    """
    
    # =========================================================================
    # HARDWARE & PATHS
    # =========================================================================
    DEVICE = "cuda"
    SEED = 42
    DATA_PATH = "data/processed/"
    
    # =========================================================================
    # DATALOADER (Optimized for i5-13500)
    # =========================================================================
    NUM_WORKERS = 6          # P-cores for parallel data loading
    PIN_MEMORY = True        # Faster CPUâ†’GPU transfer
    PREFETCH_FACTOR = 4      # Batches to prefetch per worker
    PERSISTENT_WORKERS = True  # Avoid worker restart overhead
    
    # =========================================================================
    # MIXED PRECISION (AMP)
    # =========================================================================
    USE_AMP = True           # ~25% VRAM savings, ~15% speedup
    
    # =========================================================================
    # UNIVERSAL TRAINING PARAMS
    # =========================================================================
    # Regularization-aware: smaller batches = gradient noise = implicit regularization
    BATCH_SIZE = 1024        # Reduced from 2048 for regularization effect
    EPOCHS = 100
    PATIENCE = 100           # Early stopping (generous for generative models)
    LR = 5e-4                # Lower LR for deeper model (was 1e-3)
    L2_REG = 1e-3            # Strong weight decay for high param-to-data ratio
    LR_SCHEDULER = "cosine"  # Cosine annealing
    
    # =========================================================================
    # MODEL ARCHITECTURE ("Ferrari" Upgrade)
    # =========================================================================
    # WARNING: 384 is tuned for ~13k users. Counter-balanced with high dropout.
    EMBED_DIM = 384          # Optimized: Divisible by attention heads (6, 12)
    N_LAYERS = 3             # DO NOT CHANGE. 4 layers = oversmoothing on sparse graphs.
    
    # =========================================================================
    # MODALITY PROJECTION (MLP Bridge - "Good" Weight)
    # =========================================================================
    # Instead of Linear(768 -> 384), use MLP(768 -> 1024 -> 384)
    # These params are SHARED across all items = better generalization
    PROJECTION_HIDDEN_DIM = 1024  # Wide hidden layer for non-linear mapping
    PROJECTION_DROPOUT = 0.5      # Aggressive dropout to prevent feature memorization
    
    # =========================================================================
    # EVALUATION
    # =========================================================================
    TOP_K = [10, 20, 50]     # Recall@K, NDCG@K, Precision@K
    EVAL_BATCH_SIZE = 8192   # Large (no gradients during eval)
    
    # =========================================================================
    # NEGATIVE SAMPLING
    # =========================================================================
    # 128 negatives * 384 float16 * Batch Size = VRAM monster
    N_NEGATIVES = 64          # Reduced from 128. Statistically sufficient.
    NEGATIVE_STRATEGY = "uniform"
    
    # =========================================================================
    # LATTICE SPECIFICS
    # =========================================================================
    LATTICE_K = 40           # Increased from 20 â†’ broader semantic neighborhoods
    LATTICE_LAMBDA = 0.5     # Balance original vs learned graph
    
    # =========================================================================
    # MICRO SPECIFICS
    # =========================================================================
    MICRO_TAU = 0.2          # InfoNCE temperature
    MICRO_ALPHA = 0.1        # Contrastive loss weight
    
    # =========================================================================
    # DiffMM SPECIFICS (Compute Sink - Safe to dump params here)
    # =========================================================================
    DIFFMM_STEPS = 100       # Increased from 50 â†’ higher precision generation
    DIFFMM_NOISE_SCALE = 0.1
    DIFFMM_LAMBDA_MSI = 1e-2
    DIFFMM_MLP_WIDTH = 512   # Width of internal denoising MLP
